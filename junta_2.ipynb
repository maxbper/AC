{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df1 = pd.read_csv('datasets/players.csv')\n",
    "df2 = pd.read_csv('datasets/players_teams.csv')\n",
    "df3 = pd.read_csv('datasets/awards_players.csv')\n",
    "df4 = pd.read_csv('datasets/teams.csv')\n",
    "df5 = pd.read_csv('datasets/teams_post.csv')\n",
    "df6 = pd.read_csv('datasets/coaches.csv')\n",
    "df7 = pd.read_csv('datasets/series_post.csv')\n",
    "\n",
    "def corrige_vencedor(teams, series_post):\n",
    "    # Itera sobre cada rodada ('F', 'CF', 'FR') para ajustar cada fase dos playoffs\n",
    "    for round_type in ['FR', 'CF', 'F']:\n",
    "        # Filtra a série específica da rodada\n",
    "        series_round = series_post[series_post['round'] == round_type]\n",
    "        \n",
    "        # Atualiza cada série individualmente\n",
    "        for _, row in series_round.iterrows():\n",
    "            year = row['year']\n",
    "            winner_id = row['tmIDWinner']\n",
    "            loser_id = row['tmIDLoser']\n",
    "            \n",
    "            # Define as colunas que correspondem às rodadas\n",
    "            if round_type == 'FR':\n",
    "                round_column = 'firstRound'\n",
    "            elif round_type == 'CF':\n",
    "                round_column = 'semis'\n",
    "            elif round_type == 'F':\n",
    "                round_column = 'finals'\n",
    "            \n",
    "            # Marca o time vencedor como \"W\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == winner_id), round_column] = 'W'\n",
    "            \n",
    "            # Marca o time perdedor como \"L\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == loser_id), round_column] = 'L'\n",
    "    \n",
    "    return teams\n",
    "\n",
    "teams_file = corrige_vencedor(df4, df7)\n",
    "\n",
    "players_teams_file = df2.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "\n",
    "players_file = df1[df1['pos'].notna() & (df1['pos'] != '')] # tirar jogadoras com linhas vazias\n",
    "\n",
    "players_file = players_file.drop(columns=['firstseason', 'lastseason', 'deathDate', 'collegeOther']) # dropar firstseason e lastseason porque têm os valores todos iguais, dropar deathDate porque quase ninguém morreu \n",
    "players_file['college'] = players_file['college'].apply(lambda x: 1 if pd.notnull(x) else 0) # substituir college por escolaridade\n",
    "\n",
    "merged_df = pd.merge(players_teams_file, players_file, left_on='playerID', right_on='bioID', how='left') # merge players_teams e players\n",
    "\n",
    "merged_df = merged_df.drop(columns=['bioID']) # tirar bioID porque já temos playerID\n",
    "awards_players_file = df3.drop(columns=['lgID']) # tirar porque é tudo igual\n",
    "teams_file = df4.drop(columns=['lgID', 'divID', 'tmORB','tmDRB','tmTRB','opptmORB','opptmDRB','opptmTRB','seeded']) # dropar lgID e divID porque é igual em tudo e o resto estava tudo a zero\n",
    "teams_file['playoff'] = teams_file['playoff'].apply(lambda x: 1 if x=='Y' else 0) # substituir playoff por valores numericos\n",
    "\n",
    "team_post_file = df5.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "series_post_file = df7.drop(columns=['lgIDWinner', 'lgIDLoser']) # tirar tudo porque é tudo igual\n",
    "coaches_file = df6.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "\n",
    "\n",
    "awards_grouped = awards_players_file.groupby(['playerID', 'year'])['award'].apply(list).reset_index() # agrupamos numa lista todos os awards de cada jogadora em cada ano\n",
    "awards_grouped['award'] = awards_grouped['award'].apply(lambda x: x if isinstance(x, list) else []) # se uma jogadora não tem awards -> lista vazia\n",
    "\n",
    "merged_df = pd.merge(merged_df, awards_grouped, on=['playerID', 'year'], how='left') # merge do dataset que foi merged em cima com o dos awards\n",
    "merged_df['award'] = merged_df['award'].apply(lambda x: x if isinstance(x, list) else []) # se uma jogadora não tem awards -> lista vazia\n",
    "merged_df = pd.merge(merged_df, teams_file, on=['tmID','year'], how = 'left') # merge com o teams\n",
    "\n",
    "merged_df = merged_df.drop(columns=['franchID', 'name']) # dropar franchID porque é igual à sigla da equipa e name porque já temos a sigla\n",
    "\n",
    "merged_df = pd.merge(merged_df, team_post_file, on=['tmID','year'], how = 'left') # merge com o teams post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\3116001935.py:19: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  winner_summary = winner_counts.groupby(['year', 'round']).size().reset_index(name='winner_count')\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teams = pd.read_csv('datasets/teams.csv')\n",
    "\n",
    "# Transformar os dados de rodadas em formato longo\n",
    "winner_counts = teams.melt(\n",
    "    id_vars=['year', 'tmID'],  # Colunas fixas\n",
    "    value_vars=['firstRound', 'semis', 'finals'],  # Colunas que serão transformadas\n",
    "    var_name='round',  # Nome para a coluna das rodadas\n",
    "    value_name='result'  # Nome para a coluna dos resultados\n",
    ")\n",
    "\n",
    "# Filtrar apenas as equipes vencedoras\n",
    "winner_counts = winner_counts[winner_counts['result'] == 'W']\n",
    "\n",
    "# Garantir a ordem correta das rodadas\n",
    "round_order = ['firstRound', 'semis', 'finals']\n",
    "winner_counts['round'] = pd.Categorical(winner_counts['round'], categories=round_order, ordered=True)\n",
    "\n",
    "# Contar o número de vencedores por rodada e ano\n",
    "winner_summary = winner_counts.groupby(['year', 'round']).size().reset_index(name='winner_count')\n",
    "\n",
    "# Criar o gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Cores para cada rodada\n",
    "round_colors = {\n",
    "    'firstRound': sns.color_palette('Set3')[0],\n",
    "    'semis': sns.color_palette('Set3')[1],\n",
    "    'finals': sns.color_palette('Set3')[2]\n",
    "}\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "sns.barplot(data=winner_summary, x='year', y='winner_count', hue='round', hue_order=round_order, palette=round_colors)\n",
    "\n",
    "# Adicionar linhas de valor esperado da mesma cor das barras\n",
    "plt.axhline(4, color=round_colors['firstRound'], linestyle='--', label='Valor esperado (4 vencedores - First Round)')\n",
    "plt.axhline(2, color=round_colors['semis'], linestyle='--', label='Valor esperado (2 vencedores - Semis)')\n",
    "plt.axhline(1, color=round_colors['finals'], linestyle='--', label='Valor esperado (1 vencedor - Finals)')\n",
    "\n",
    "# Personalizar o gráfico\n",
    "plt.title('Número de vencedores por ronda e ano')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Número de vencedores')\n",
    "plt.legend(title='Expectativa por ronda')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibir o gráfico\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min, max, and average player count for each year\n",
    "\n",
    "player_count_per_team_year = merged_df.groupby(['year', 'tmID'])['playerID'].nunique().reset_index()\n",
    "player_count_per_team_year.columns = ['Year', 'Team', 'PlayerCount']\n",
    "\n",
    "# print(player_count_per_team_year)\n",
    "\n",
    "summary_stats = player_count_per_team_year.groupby('Year')['PlayerCount'].agg(['min', 'max', 'mean']).reset_index()\n",
    "summary_stats.columns = ['Year', 'MinPlayerCount', 'MaxPlayerCount', 'AvgPlayerCount']\n",
    "\n",
    "#summary_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge do awards com os coaches\n",
    "\n",
    "awards_coaches_file = df3.rename(columns={'playerID': 'coachID'})\n",
    "coach_awards = awards_coaches_file[awards_coaches_file['award'] == 'Coach of the Year']\n",
    "coach_awards_grouped = coach_awards.groupby(['coachID', 'year'])['award'].apply(list).reset_index()\n",
    "coaches_file = pd.merge(coaches_file, coach_awards_grouped, on=['coachID', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rebounds by position, excluding secondary positions\n",
    "avg_rebounds_by_pos_filtered = (\n",
    "    merged_df.groupby('pos')\n",
    "    .agg(oRebounds=('oRebounds', 'mean'), dRebounds=('dRebounds', 'mean'))\n",
    "    .reset_index()\n",
    "    .query(\"pos in ['G', 'F', 'C']\")\n",
    ")\n",
    "\n",
    "avg_rebounds_by_pos_filtered['d_over_o_reb'] = avg_rebounds_by_pos_filtered['dRebounds'] / avg_rebounds_by_pos_filtered['oRebounds']\n",
    "#avg_rebounds_by_pos_filtered.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['rebounds', 'PostRebounds']) # resultado da célula anterior, os rebounds ofensivos/defensivos mudam de acordo com a posição da jogadora, logo decidimos tirar o total de rebounds\n",
    "merged_df = merged_df.rename(columns={'GP_x': 'GP_player', 'GP_y': 'GP_team'}) # haviam duas colunas com nomes iguais, uma para as jogadoras e outra para as equipas, tinham ficado uma com x e a outra com y então demos rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Novas estatisticas -> PER (Player Effiency Rating)\n",
    "\n",
    "grouped = merged_df.groupby('year').agg({\n",
    "    'o_pts': 'sum',\n",
    "    'o_fga': 'sum',\n",
    "    'o_oreb': 'sum',\n",
    "    'o_to': 'sum',\n",
    "    'o_fta': 'sum',\n",
    "    'o_asts': 'sum',\n",
    "    'o_fgm' : 'sum',\n",
    "    'o_ftm': 'sum',\n",
    "    'o_dreb':'sum',\n",
    "}).reset_index()\n",
    "\n",
    "grouped['VOP'] = grouped['o_pts'] / (grouped['o_fga'] - grouped['o_oreb'] + grouped['o_to'] + 0.44 * grouped['o_fta'])\n",
    "grouped['factor'] = (2 / 3) - (0.5 * (grouped['o_asts'] / grouped['o_fgm'])) / (2 * (grouped['o_fgm'] / grouped['o_ftm']))\n",
    "grouped['DRB%'] = (grouped['o_dreb'] - grouped['o_oreb']) / grouped['o_dreb']\n",
    "\n",
    "uPER_df = merged_df.groupby(['playerID', 'year']).agg({\n",
    "    'minutes': 'sum',     \n",
    "    'threeMade': 'sum',   \n",
    "    'assists': 'sum',     \n",
    "    'fgMade': 'sum',      \n",
    "    'ftMade': 'sum',      \n",
    "    'turnovers': 'sum',   \n",
    "    'fgAttempted': 'sum', \n",
    "    'ftAttempted': 'sum', \n",
    "    'dRebounds': 'sum',   \n",
    "    'oRebounds': 'sum',   \n",
    "    'steals': 'sum',      \n",
    "    'blocks': 'sum',      \n",
    "    'PF': 'sum'           \n",
    "}).reset_index()\n",
    "\n",
    "uPER_df = uPER_df.merge(grouped[['year', 'VOP', 'factor', 'DRB%']], on='year')\n",
    "\n",
    "uPER_df['TRB'] = uPER_df['dRebounds'] + uPER_df['oRebounds']\n",
    "\n",
    "uPER_df['uPER'] = (1 / uPER_df['minutes']) * (\n",
    "    uPER_df['threeMade'] +\n",
    "    (2/3) * uPER_df['assists'] +\n",
    "    (2 - uPER_df['factor'] * (uPER_df['assists'] / uPER_df['fgMade'])) * uPER_df['fgMade'] +\n",
    "    (uPER_df['ftMade'] * 0.5 * (1 + (1 - (uPER_df['assists'] / uPER_df['fgMade'])) + (2/3) * (uPER_df['assists'] / uPER_df['fgMade']))) -\n",
    "    uPER_df['VOP'] * uPER_df['turnovers'] -\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * (uPER_df['fgAttempted'] - uPER_df['fgMade']) -\n",
    "    uPER_df['VOP'] * 0.44 * (0.44 + (0.56 * uPER_df['DRB%'])) * (uPER_df['ftAttempted'] - uPER_df['ftMade']) +\n",
    "    uPER_df['VOP'] * (1 - uPER_df['DRB%']) * uPER_df['TRB'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['oRebounds'] +\n",
    "    uPER_df['VOP'] * uPER_df['steals'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['blocks'] -\n",
    "    uPER_df['PF'] * ((grouped['o_ftm'].mean() / grouped['o_pts'].mean()) - 0.44 * (grouped['o_fta'].mean() / grouped['o_pts'].mean()) * uPER_df['VOP'])\n",
    ")\n",
    "\n",
    "lg_uPER = uPER_df.groupby('year')['uPER'].mean().reset_index()\n",
    "lg_uPER.rename(columns={'uPER': 'lg_uPER'}, inplace=True)\n",
    "\n",
    "uPER_df = uPER_df.merge(lg_uPER, on='year')\n",
    "\n",
    "uPER_df['PER'] = uPER_df['uPER'] * (15 / uPER_df['lg_uPER'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_to_merge = uPER_df[['playerID', 'year', 'PER']]\n",
    "merged_df = merged_df.merge(per_to_merge, on=['playerID', 'year'], how='left') # adicionar o PER ao dataset\n",
    "\n",
    "# novas estatisticas\n",
    "merged_df['TS%'] = (merged_df['points'] / (2 * (merged_df['fgAttempted'] + 0.44 * merged_df['ftAttempted'])))*100\n",
    "merged_df['eFG%'] = ((merged_df['fgMade'] + 0.5 * merged_df['threeMade']) / merged_df['fgAttempted'])*100\n",
    "merged_df['stocks'] = (merged_df['steals'] + merged_df['blocks'])\n",
    "merged_df['dar'] = ((merged_df['steals'] + merged_df['blocks'] + merged_df['oRebounds'] + merged_df['dRebounds'])/merged_df['minutes'])\n",
    "\n",
    "# substituir por 0 colunas vazias\n",
    "merged_df['PER'] = merged_df['PER'].fillna(0)\n",
    "merged_df['TS%'] = merged_df['TS%'].fillna(0)\n",
    "merged_df['eFG%'] = merged_df['eFG%'].fillna(0)\n",
    "merged_df['stocks'] = merged_df['stocks'].fillna(0)\n",
    "merged_df['dar'] = merged_df['dar'].fillna(0)\n",
    "\n",
    "#TODO escalar pelos minutos jogados (per, efg stocks e ts pelo tempo. O dar nao pois já tem em conta os minutos jogados)\n",
    "max_minutes = merged_df['minutes'].max()\n",
    "\n",
    "if max_minutes != 0:\n",
    "    # Multiplicando cada valor de 'TS%' pelo fator (min_minutes / max_minutes)\n",
    "    merged_df['TS%'] = merged_df['TS%'] * (merged_df['minutes'] / max_minutes)\n",
    "    merged_df['eFG%'] = merged_df['eFG%'] * (merged_df['minutes'] / max_minutes)\n",
    "    merged_df['PER'] = merged_df['PER'] * (merged_df['minutes'] / max_minutes)\n",
    "    merged_df['stocks'] = merged_df['stocks'] * (merged_df['minutes'] / max_minutes)\n",
    "\n",
    "#TODO............................................\n",
    "\n",
    "\n",
    "#Equipas que não foram aos playoffs\n",
    "merged_df['W'] = merged_df['W'].fillna(0)\n",
    "merged_df['L'] = merged_df['L'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_for_each_column(dataset):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    if numeric_columns.empty:\n",
    "        print(\"No numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        numeric_columns.boxplot(figsize=(10, 6))\n",
    "        plt.title(\"Boxplot for all numeric columns\")\n",
    "        plt.xticks(rotation=45)  # Rotation in x, if necessary\n",
    "        plt.show()\n",
    "\n",
    "def pearson_correlation(dataset, size_x, size_y):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    \n",
    "    if numeric_columns.empty:\n",
    "        print(\"Nenhuma coluna numérica encontrada no dataset.\")\n",
    "    else:\n",
    "        # Correlation matrix\n",
    "        correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "        # View\n",
    "        plt.figure(figsize=(size_x, size_y))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Pearson-correlation')\n",
    "        plt.show()\n",
    "\n",
    "def bar_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            value_counts = non_numeric_columns[column].value_counts()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f\"Bar chart for '{column}'\")\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "            plt.show()\n",
    "\n",
    "# Pie-chart for each column\n",
    "def pie_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    \n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            # Count elements from different categories\n",
    "            category_counts = dataset[column].value_counts()\n",
    "            \n",
    "            # Pie-chart\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            category_counts.plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "            plt.title(f'Distribution of {column}')\n",
    "            plt.ylabel('')  # Remove o rótulo do eixo Y\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#box_plot_for_each_column(merged_df)\n",
    "#box_plot_for_each_column(coaches_file)\n",
    "#box_plot_for_each_column(series_post_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearson_correlation(merged_df, 100, 80)\n",
    "#pearson_correlation(coaches_file, 8, 6)\n",
    "#pearson_correlation(series_post_file, 8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar_chart_for_each_column(merged_df)\n",
    "#bar_chart_for_each_column(coaches_file)\n",
    "#bar_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie_chart_for_each_column(merged_df)\n",
    "#pie_chart_for_each_column(coaches_file)\n",
    "#pie_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir valores nominais para valores relativos (one-attribute-per-value conversion)\n",
    "def replaceGameResults(column):\n",
    "    return column.apply(lambda value: '100' if value == 'W' else '010' if value == 'L' else '001')\n",
    "\n",
    "# Aplicar a função para cada coluna específica\n",
    "merged_df['firstRound'] = replaceGameResults(merged_df['firstRound'])\n",
    "merged_df['semis'] = replaceGameResults(merged_df['semis'])\n",
    "merged_df['finals'] = replaceGameResults(merged_df['finals'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar os datasets numa pasta\n",
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df.to_csv('cleanDatasets/players_and_teams.csv', index=False)\n",
    "coaches_file.to_csv('cleanDatasets/coaches_and_awards.csv', index=False)\n",
    "series_post_file.to_csv('cleanDatasets/series_post.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queriamos construir o dataset em relação às jogadoras então dropamos imensas colunas relativas à equipa\n",
    "\n",
    "merged_df2 = merged_df.drop(columns=['minutes','threeMade','assists','fgMade','turnovers','fgAttempted','ftAttempted','oRebounds','steals','blocks','PF','o_ftm','o_pts','o_fta','o_pts','o_fga','o_oreb','o_to','o_asts','o_fgm','o_dreb']) # estes atributos já se encontram nas novas colunas criadas\n",
    "merged_df2 = merged_df2.drop(columns=['GP_player','ftMade','threeAttempted','GP_team'])\n",
    "merged_df2 = merged_df2.drop(columns=['o_3pm','o_3pa','o_reb','o_pf','o_stl','o_blk','d_fgm','d_fga','d_ftm','d_fta','d_3pm','d_3pa','d_oreb','d_dreb','d_reb','d_asts','d_pf','d_stl','d_to','d_blk','d_pts'])\n",
    "merged_df2 = merged_df2.drop(columns=['PostGP','PostGS','PostMinutes','PostPoints','PostoRebounds','PostdRebounds','PostAssists','PostSteals','PostBlocks','PostTurnovers','PostPF','PostfgAttempted','PostfgMade','PostftAttempted','PostftMade','PostthreeAttempted','PostthreeMade','PostDQ'])\n",
    "merged_df2 = merged_df2.drop(columns=['arena'])\n",
    "\n",
    "# substituir BirthDate por ano em que nasceram\n",
    "merged_df2['birthDate'] = pd.to_datetime(merged_df['birthDate'], errors='coerce').dt.year\n",
    "merged_df2 = merged_df2.rename(columns={'birthDate': 'birthYear'})\n",
    "\n",
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df2.to_csv('cleanDatasets/advancedstatistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playerID</th>\n",
       "      <th>year</th>\n",
       "      <th>stint</th>\n",
       "      <th>tmID</th>\n",
       "      <th>GS</th>\n",
       "      <th>points</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dq</th>\n",
       "      <th>pos</th>\n",
       "      <th>height</th>\n",
       "      <th>...</th>\n",
       "      <th>min</th>\n",
       "      <th>attend</th>\n",
       "      <th>W</th>\n",
       "      <th>L</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>stocks</th>\n",
       "      <th>dar</th>\n",
       "      <th>coachID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abrossv01w</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>23</td>\n",
       "      <td>343</td>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>F</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6475</td>\n",
       "      <td>120607</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.226985</td>\n",
       "      <td>33.489853</td>\n",
       "      <td>28.897119</td>\n",
       "      <td>34.964344</td>\n",
       "      <td>0.265957</td>\n",
       "      <td>[aglerbr99w]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abrossv01w</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>27</td>\n",
       "      <td>314</td>\n",
       "      <td>101</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6475</td>\n",
       "      <td>139874</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.371122</td>\n",
       "      <td>27.904033</td>\n",
       "      <td>26.630747</td>\n",
       "      <td>33.922204</td>\n",
       "      <td>0.245963</td>\n",
       "      <td>[aglerbr99w, vandehe99w]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abrossv01w</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>25</td>\n",
       "      <td>318</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6850</td>\n",
       "      <td>120253</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.864571</td>\n",
       "      <td>31.101006</td>\n",
       "      <td>28.037192</td>\n",
       "      <td>35.299838</td>\n",
       "      <td>0.247475</td>\n",
       "      <td>[mcconsu01w]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abrossv01w</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>11</td>\n",
       "      <td>146</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6850</td>\n",
       "      <td>125097</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.445754</td>\n",
       "      <td>17.163170</td>\n",
       "      <td>15.891468</td>\n",
       "      <td>11.980551</td>\n",
       "      <td>0.229437</td>\n",
       "      <td>[mcconsu01w]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abrossv01w</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>31</td>\n",
       "      <td>304</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>F</td>\n",
       "      <td>74.0</td>\n",
       "      <td>...</td>\n",
       "      <td>6850</td>\n",
       "      <td>113447</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.388236</td>\n",
       "      <td>31.062010</td>\n",
       "      <td>28.631263</td>\n",
       "      <td>34.001621</td>\n",
       "      <td>0.207207</td>\n",
       "      <td>[mcconsu01w]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     playerID  year  stint tmID  GS  points  dRebounds  dq pos  height  ...  \\\n",
       "0  abrossv01w     2      0  MIN  23     343        131   2   F    74.0  ...   \n",
       "1  abrossv01w     3      0  MIN  27     314        101   0   F    74.0  ...   \n",
       "2  abrossv01w     4      0  MIN  25     318         97   0   F    74.0  ...   \n",
       "3  abrossv01w     5      0  MIN  11     146         57   0   F    74.0  ...   \n",
       "4  abrossv01w     6      0  MIN  31     304         78   0   F    74.0  ...   \n",
       "\n",
       "    min  attend    W    L        PER        TS%       eFG%     stocks  \\\n",
       "0  6475  120607  0.0  0.0  13.226985  33.489853  28.897119  34.964344   \n",
       "1  6475  139874  0.0  0.0  10.371122  27.904033  26.630747  33.922204   \n",
       "2  6850  120253  1.0  2.0  11.864571  31.101006  28.037192  35.299838   \n",
       "3  6850  125097  0.0  2.0   5.445754  17.163170  15.891468  11.980551   \n",
       "4  6850  113447  0.0  0.0  11.388236  31.062010  28.631263  34.001621   \n",
       "\n",
       "        dar                   coachID  \n",
       "0  0.265957              [aglerbr99w]  \n",
       "1  0.245963  [aglerbr99w, vandehe99w]  \n",
       "2  0.247475              [mcconsu01w]  \n",
       "3  0.229437              [mcconsu01w]  \n",
       "4  0.207207              [mcconsu01w]  \n",
       "\n",
       "[5 rows x 38 columns]"
      ]
     },
     "execution_count": 573,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df9 = pd.read_csv('datasetsYear11/players_teams.csv')\n",
    "df8 = pd.read_csv('datasetsYear11/coaches.csv')\n",
    "df10 = pd.read_csv('datasetsYear11/teams.csv')\n",
    "\n",
    "# Agrupar coaches por year e team, transformando em listas\n",
    "coaches_grouped = df6.groupby(['year', 'tmID'])['coachID'].apply(list).reset_index()\n",
    "\n",
    "# Fazer o merge com o ficheiro\n",
    "merged_df2 = merged_df2.merge(coaches_grouped, on=['year', 'tmID'], how='left')\n",
    "\n",
    "year11 = df9.merge(df8, on=['tmID','year'], how='left')\n",
    "year11 = year11.merge(df10, on=['tmID','year'], how='left')\n",
    "finalYear11 = year11[['playerID','year','tmID','confID','coachID']]\n",
    "finalYear11 = finalYear11.groupby(['playerID', 'year', 'tmID', 'confID'])['coachID'].apply(list).reset_index()\n",
    "\n",
    "# Adicionar colunas ausentes em finalYear11 com valor 0\n",
    "expected_columns_merged_df2 = merged_df2.columns\n",
    "finalYear11 = finalYear11.reindex(columns=expected_columns_merged_df2, fill_value=0)\n",
    "\n",
    "# Concatenar os datasets\n",
    "merged_df2 = pd.concat([merged_df2, finalYear11], ignore_index=True)\n",
    "\n",
    "merged_df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_stats_prevYear = merged_df2[['playerID','year','PER', 'eFG%', 'TS%','stocks','dRebounds','dar', 'GS']].drop_duplicates().copy() #TODO Acrescentar aqui mais variaveis\n",
    "\n",
    "#players_stats_prevYear['legacy_points'] = merged_df2['points']\n",
    "players_stats_prevYear['legacy_points'] = merged_df2['won']\n",
    "players_stats_prevYear['year'] = players_stats_prevYear['year'] + 1\n",
    "\n",
    "\n",
    "players_stats_prevYear = players_stats_prevYear.merge(\n",
    "    merged_df2[['playerID', 'year', 'tmID', 'playoff','coachID','confID']], \n",
    "    on=['playerID', 'year'], \n",
    "    how='left')\n",
    "\n",
    "players_stats_prevYear.to_csv('cleanDatasets/players_stats_prevYear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmID</th>\n",
       "      <th>year</th>\n",
       "      <th>playoff</th>\n",
       "      <th>confID</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>stocks</th>\n",
       "      <th>dRebounds</th>\n",
       "      <th>dar</th>\n",
       "      <th>GS</th>\n",
       "      <th>coachID</th>\n",
       "      <th>legacy_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATL</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>EA</td>\n",
       "      <td>7.130778</td>\n",
       "      <td>22.487863</td>\n",
       "      <td>20.972337</td>\n",
       "      <td>15.411669</td>\n",
       "      <td>57.111111</td>\n",
       "      <td>0.221307</td>\n",
       "      <td>10.777778</td>\n",
       "      <td>[meadoma99w]</td>\n",
       "      <td>18.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EA</td>\n",
       "      <td>8.956513</td>\n",
       "      <td>26.740715</td>\n",
       "      <td>24.489119</td>\n",
       "      <td>22.914641</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>0.257224</td>\n",
       "      <td>17.333333</td>\n",
       "      <td>[meadoma99w]</td>\n",
       "      <td>8.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATL</td>\n",
       "      <td>11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EA</td>\n",
       "      <td>10.468933</td>\n",
       "      <td>31.506456</td>\n",
       "      <td>29.149805</td>\n",
       "      <td>31.930308</td>\n",
       "      <td>89.888889</td>\n",
       "      <td>0.233274</td>\n",
       "      <td>19.444444</td>\n",
       "      <td>[meadoma99w]</td>\n",
       "      <td>17.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CHA</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EA</td>\n",
       "      <td>8.214271</td>\n",
       "      <td>26.280398</td>\n",
       "      <td>23.619522</td>\n",
       "      <td>20.759139</td>\n",
       "      <td>54.111111</td>\n",
       "      <td>0.189456</td>\n",
       "      <td>13.555556</td>\n",
       "      <td>[donovan99w]</td>\n",
       "      <td>14.444444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CHA</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>EA</td>\n",
       "      <td>8.376030</td>\n",
       "      <td>26.572410</td>\n",
       "      <td>23.977519</td>\n",
       "      <td>21.446155</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.202011</td>\n",
       "      <td>16.888889</td>\n",
       "      <td>[donovan99w]</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  tmID  year  playoff confID        PER        TS%       eFG%     stocks  \\\n",
       "0  ATL     9      0.0     EA   7.130778  22.487863  20.972337  15.411669   \n",
       "1  ATL    10      1.0     EA   8.956513  26.740715  24.489119  22.914641   \n",
       "2  ATL    11      1.0     EA  10.468933  31.506456  29.149805  31.930308   \n",
       "3  CHA     2      1.0     EA   8.214271  26.280398  23.619522  20.759139   \n",
       "4  CHA     3      1.0     EA   8.376030  26.572410  23.977519  21.446155   \n",
       "\n",
       "   dRebounds       dar         GS       coachID  legacy_points  \n",
       "0  57.111111  0.221307  10.777778  [meadoma99w]      18.777778  \n",
       "1  76.000000  0.257224  17.333333  [meadoma99w]       8.444444  \n",
       "2  89.888889  0.233274  19.444444  [meadoma99w]      17.666667  \n",
       "3  54.111111  0.189456  13.555556  [donovan99w]      14.444444  \n",
       "4  60.000000  0.202011  16.888889  [donovan99w]      18.000000  "
      ]
     },
     "execution_count": 575,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.............................Fazer a media por equipa dos valores mas pegando apenas nos 7 melhores jogadores...............\n",
    "\n",
    "# Ordenar os jogadores dentro de cada equipe e ano com base no PER (ou outra métrica)\n",
    "players_stats_prevYear_sorted = players_stats_prevYear.sort_values(by=['tmID', 'year', 'PER'], ascending=[True, True, False])\n",
    " \n",
    "#TODO NAO USAR APENAS 5 jogadores, usar todos da equipa\n",
    "# Selecionar os 5 melhores jogadores de cada equipe e ano\n",
    "top_7_players = players_stats_prevYear_sorted.groupby(['tmID', 'year']).head(9)\n",
    "\n",
    "# Agora, calcular a média das métricas apenas para os melhores jogadores\n",
    "team_year_stats = top_7_players.groupby(['tmID', 'year', 'playoff','confID']).agg({\n",
    "    'PER': 'mean',\n",
    "    'TS%': 'mean',\n",
    "    'eFG%': 'mean',\n",
    "    'stocks': 'mean',\n",
    "    'dRebounds': 'mean',\n",
    "    'dar': 'mean',\n",
    "    'GS': 'mean',\n",
    "    'coachID': 'first',  # Seleciona o primeiro valor da coluna (presumindo que todos sejam iguais para o grupo)\n",
    "    'legacy_points' : 'mean' #Same here\n",
    "}).reset_index()\n",
    "\n",
    "#TODO...........................................................\n",
    "#playoff_teams_year11 = [\"WAS\", \"NYL\", \"IND\", \"ATL\", \"SEA\", \"PHO\", \"SAS\", \"LAS\"]\n",
    "playoff_teams_year11 = [\"WAS\", \"NYL\", \"IND\", \"ATL\", \"SEA\", \"PHO\", \"SAS\", \"CON\"]\n",
    "\n",
    "team_year_stats.loc[(team_year_stats['year'] == 11) & (team_year_stats['tmID'].isin(playoff_teams_year11)), 'playoff'] = 1\n",
    "# Salva o novo dataset em um arquivo CSV\n",
    "team_year_stats.to_csv('cleanDatasets/team_year_stats.csv', index=False)\n",
    "\n",
    "team_year_stats.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stocks', 'dar', 'eFG%', 'GS', 'legacy_points']\n",
      "    tmID  year  playoff confID       eFG%     stocks       dar         GS  \\\n",
      "0    ATL     9      0.0     EA  20.972337  15.411669  0.221307  10.777778   \n",
      "1    ATL    10      1.0     EA  24.489119  22.914641  0.257224  17.333333   \n",
      "2    ATL    11      1.0     EA  29.149805  31.930308  0.233274  19.444444   \n",
      "3    CHA     2      1.0     EA  23.619522  20.759139  0.189456  13.555556   \n",
      "4    CHA     3      1.0     EA  23.977519  21.446155  0.202011  16.888889   \n",
      "..   ...   ...      ...    ...        ...        ...       ...        ...   \n",
      "133  WAS     7      1.0     EA  27.033872  24.662075  0.172187  18.500000   \n",
      "134  WAS     8      0.0     EA  32.354406  34.295876  0.223793  23.222222   \n",
      "135  WAS     9      0.0     EA  25.818150  26.740501  0.232656  15.444444   \n",
      "136  WAS    10      1.0     EA  23.814309  21.329372  0.229763  17.222222   \n",
      "137  WAS    11      1.0     EA  27.300987  30.179092  0.254096  20.875000   \n",
      "\n",
      "                      coachID  legacy_points  \n",
      "0                [meadoma99w]      18.777778  \n",
      "1                [meadoma99w]       8.444444  \n",
      "2                [meadoma99w]      17.666667  \n",
      "3                [donovan99w]      14.444444  \n",
      "4                [donovan99w]      18.000000  \n",
      "..                        ...            ...  \n",
      "133              [adubari99w]      16.375000  \n",
      "134  [adubari99w, rollitr01w]      15.777778  \n",
      "135  [kenlaje99w, rollitr01w]      14.333333  \n",
      "136              [plankju99w]      13.000000  \n",
      "137              [laceytr99w]      15.000000  \n",
      "\n",
      "[138 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Para depois poder ir buscar as variavies categoricas\n",
    "team_year_stats_copy = team_year_stats.copy()\n",
    "\n",
    "\n",
    "# Selecionar as colunas de interesse\n",
    "features = team_year_stats[['PER', 'TS%', 'eFG%','stocks','dar']] #TODO Acrescentar aqui tambem\n",
    "\n",
    "# Normalizar os dados usando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Aplicar o PCA\n",
    "pca = PCA(n_components=3) #TODO mudar aqui o numero de colunas a selecionar\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Verificar as cargas (coeficientes) dos componentes principais\n",
    "components = pca.components_\n",
    "\n",
    "# Baseado nas cargas, você pode decidir as variáveis mais importantes\n",
    "# Vamos mostrar a importância de cada variável nas componentes principais\n",
    "\n",
    "# Calcular a soma das cargas absolutas para cada variável\n",
    "importance = pd.DataFrame(abs(components), columns=['PER', 'TS%', 'eFG%','stocks','dar'], index=['PC1', 'PC2','PC3']) #TODO mudar aqui as variaveis\n",
    "#TODO meter tantos PC quanto variaves a selecionar\n",
    "importance_sum = importance.sum(axis=0)\n",
    "\n",
    "# Selecionar as duas variáveis mais importantes\n",
    "most_important_features = importance_sum.sort_values(ascending=False).head(3) #TODO mudar aqui o numero de variaveis\n",
    "\n",
    "# Exibir apenas os nomes das variáveis mais importantes\n",
    "important_variable_names = most_important_features.index.tolist()\n",
    "\n",
    "# Inicializar a lista de componentes a serem removidos\n",
    "components_to_drop = ['PER', 'TS%', 'eFG%','stocks','dRebounds','dar'] #TODO mudar aqui as variavies (TODOS OS ATRIBUTOS AQUI)\n",
    "#TODO nao esquecer de aqui adicionar as nao numericas, como os coaches\n",
    "\n",
    "# Remover as variáveis mais importantes da lista de componentes a serem removidos\n",
    "components_to_drop = [col for col in components_to_drop if col not in important_variable_names]\n",
    "\n",
    "team_year_stats=team_year_stats.drop(columns=components_to_drop)\n",
    "\n",
    "#Adicionar as variaveis categoricas, como os coaches, que nao passaram no processo de PCA\n",
    "#team_year_stats['coachID'] = team_year_stats_copy['coachID']\n",
    "team_year_stats['legacy_points'] = team_year_stats_copy['legacy_points']\n",
    "team_year_stats['GS'] = team_year_stats_copy['GS']\n",
    "important_variable_names.append('GS')\n",
    "important_variable_names.append('legacy_points')\n",
    "\n",
    "# Exibir o resultado\n",
    "print(important_variable_names)\n",
    "print(team_year_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tmID  year  playoff confID      eFG%    stocks       dar        GS  \\\n",
      "0  ATL     9      0.0     EA  0.284330  0.188426  0.592166  0.214559   \n",
      "1  ATL    10      1.0     EA  0.444426  0.385830  0.845975  0.497126   \n",
      "2  ATL    11      1.0     EA  0.656596  0.623033  0.676729  0.588123   \n",
      "3  CHA     2      1.0     EA  0.404839  0.329119  0.367090  0.334291   \n",
      "4  CHA     3      1.0     EA  0.421136  0.347194  0.455807  0.477969   \n",
      "\n",
      "        coachID  legacy_points  \n",
      "0  [meadoma99w]       0.586592  \n",
      "1  [meadoma99w]       0.067039  \n",
      "2  [meadoma99w]       0.530726  \n",
      "3  [donovan99w]       0.368715  \n",
      "4  [donovan99w]       0.547486  \n"
     ]
    }
   ],
   "source": [
    "# Normalizar os dados \n",
    "\n",
    "# Selecionar apenas colunas numéricas\n",
    "# Lista de colunas a normalizar\n",
    "columns_to_normalize = ['eFG%', 'stocks','dar','legacy_points', 'GS'] #TODO baseado nas colunas selecionadas do dataset de cima\n",
    "\n",
    "# Aplicar a normalização apenas nas colunas selecionadas\n",
    "team_year_stats[columns_to_normalize] = team_year_stats[columns_to_normalize].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")\n",
    "\n",
    "# Verificar o resultado\n",
    "print(team_year_stats.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" #Passar as colunas que nao sao numericas para numericas\\n# One-Hot Encoding e é útil para transformar colunas categóricas em representações numéricas binárias.\\n\\nfrom sklearn.preprocessing import MultiLabelBinarizer\\n\\nmlb = MultiLabelBinarizer()\\n\\n# Codificando a lista de coaches em colunas binárias\\ncoach_dummies = mlb.fit_transform(team_year_stats['coachID'])\\n\\n# Adicionando as novas colunas no dataframe original\\ncoach_columns = mlb.classes_\\nteam_year_stats = team_year_stats.join(pd.DataFrame(coach_dummies, columns=coach_columns))\\n\\n# Adicionando os nomes das novas colunas à lista de variáveis importantes\\nimportant_variable_names.extend(coach_columns) \""
      ]
     },
     "execution_count": 578,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" #Passar as colunas que nao sao numericas para numericas\n",
    "# One-Hot Encoding e é útil para transformar colunas categóricas em representações numéricas binárias.\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Codificando a lista de coaches em colunas binárias\n",
    "coach_dummies = mlb.fit_transform(team_year_stats['coachID'])\n",
    "\n",
    "# Adicionando as novas colunas no dataframe original\n",
    "coach_columns = mlb.classes_\n",
    "team_year_stats = team_year_stats.join(pd.DataFrame(coach_dummies, columns=coach_columns))\n",
    "\n",
    "# Adicionando os nomes das novas colunas à lista de variáveis importantes\n",
    "important_variable_names.extend(coach_columns) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir o dataset para treino, validacao e teste \n",
    "\n",
    "dataset_treino = team_year_stats[(team_year_stats['year'] >= 0) & (team_year_stats['year'] <= 10)]\n",
    "dataset_teste = team_year_stats[team_year_stats['year'] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\699143788.py:16: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_scaled = df.applymap(lambda x: 1 if x > 0.85 else x)\n",
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\699143788.py:17: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df_scaled2 = df_scaled.applymap(lambda x: 0 if x < 0.15 else x)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>stocks</th>\n",
       "      <th>dar</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>GS</th>\n",
       "      <th>legacy_points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.188426</td>\n",
       "      <td>0.592166</td>\n",
       "      <td>0.284330</td>\n",
       "      <td>0.214559</td>\n",
       "      <td>0.586592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.385830</td>\n",
       "      <td>0.845975</td>\n",
       "      <td>0.444426</td>\n",
       "      <td>0.497126</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.329119</td>\n",
       "      <td>0.367090</td>\n",
       "      <td>0.404839</td>\n",
       "      <td>0.334291</td>\n",
       "      <td>0.368715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.347194</td>\n",
       "      <td>0.455807</td>\n",
       "      <td>0.421136</td>\n",
       "      <td>0.477969</td>\n",
       "      <td>0.547486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.403835</td>\n",
       "      <td>0.388869</td>\n",
       "      <td>0.518714</td>\n",
       "      <td>0.516284</td>\n",
       "      <td>0.536313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.475337</td>\n",
       "      <td>0.532902</td>\n",
       "      <td>0.437742</td>\n",
       "      <td>0.552802</td>\n",
       "      <td>0.553771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.739150</td>\n",
       "      <td>0.356975</td>\n",
       "      <td>0.836505</td>\n",
       "      <td>0.837165</td>\n",
       "      <td>0.446927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.362237</td>\n",
       "      <td>0.515985</td>\n",
       "      <td>0.237400</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.150818</td>\n",
       "      <td>0.473320</td>\n",
       "      <td>0.194093</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.452514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.338599</td>\n",
       "      <td>0.441769</td>\n",
       "      <td>0.268086</td>\n",
       "      <td>0.564176</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.409172</td>\n",
       "      <td>0.678562</td>\n",
       "      <td>0.387781</td>\n",
       "      <td>0.391762</td>\n",
       "      <td>0.396648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.538377</td>\n",
       "      <td>0.678031</td>\n",
       "      <td>0.508063</td>\n",
       "      <td>0.545019</td>\n",
       "      <td>0.256983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.326527</td>\n",
       "      <td>0.450113</td>\n",
       "      <td>0.491564</td>\n",
       "      <td>0.372605</td>\n",
       "      <td>0.553073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.295830</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.322832</td>\n",
       "      <td>0.324713</td>\n",
       "      <td>0.748603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.296109</td>\n",
       "      <td>0.455763</td>\n",
       "      <td>0.392262</td>\n",
       "      <td>0.463602</td>\n",
       "      <td>0.201117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.512062</td>\n",
       "      <td>0.550983</td>\n",
       "      <td>0.522046</td>\n",
       "      <td>0.597701</td>\n",
       "      <td>0.402235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.565793</td>\n",
       "      <td>0.649346</td>\n",
       "      <td>0.595971</td>\n",
       "      <td>0.511494</td>\n",
       "      <td>0.472067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.688164</td>\n",
       "      <td>0.554373</td>\n",
       "      <td>0.530211</td>\n",
       "      <td>0.501916</td>\n",
       "      <td>0.497207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.509748</td>\n",
       "      <td>0.598180</td>\n",
       "      <td>0.487486</td>\n",
       "      <td>0.554598</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.457270</td>\n",
       "      <td>0.705334</td>\n",
       "      <td>0.469979</td>\n",
       "      <td>0.487548</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      stocks       dar      eFG%        GS  legacy_points\n",
       "0   0.188426  0.592166  0.284330  0.214559       0.586592\n",
       "1   0.385830  0.845975  0.444426  0.497126       0.000000\n",
       "3   0.329119  0.367090  0.404839  0.334291       0.368715\n",
       "4   0.347194  0.455807  0.421136  0.477969       0.547486\n",
       "5   0.403835  0.388869  0.518714  0.516284       0.536313\n",
       "6   0.475337  0.532902  0.437742  0.552802       0.553771\n",
       "7   0.739150  0.356975  0.836505  0.837165       0.446927\n",
       "8   0.362237  0.515985  0.237400  0.444444       0.000000\n",
       "9   0.150818  0.473320  0.194093  0.166667       0.452514\n",
       "10  0.338599  0.441769  0.268086  0.564176       0.000000\n",
       "11  0.409172  0.678562  0.387781  0.391762       0.396648\n",
       "12  0.538377  0.678031  0.508063  0.545019       0.256983\n",
       "14  0.326527  0.450113  0.491564  0.372605       0.553073\n",
       "15  0.295830  1.000000  0.322832  0.324713       0.748603\n",
       "16  0.296109  0.455763  0.392262  0.463602       0.201117\n",
       "17  0.512062  0.550983  0.522046  0.597701       0.402235\n",
       "18  0.565793  0.649346  0.595971  0.511494       0.472067\n",
       "19  0.688164  0.554373  0.530211  0.501916       0.497207\n",
       "20  0.509748  0.598180  0.487486  0.554598       1.000000\n",
       "21  0.457270  0.705334  0.469979  0.487548       1.000000"
      ]
     },
     "execution_count": 580,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "def scale_values(df):\n",
    "    df_scaled = df.applymap(lambda x: 1 if x > 0.85 else x)\n",
    "    df_scaled2 = df_scaled.applymap(lambda x: 0 if x < 0.15 else x)\n",
    "    return df_scaled2\n",
    "\n",
    "# Features (X) e alvo (y)\n",
    "X_treino = dataset_treino[important_variable_names] #Antigamente 'PER', 'TS%', 'eFG%'\n",
    "y_treino = dataset_treino['playoff']\n",
    "\n",
    "X_teste = dataset_teste[important_variable_names]\n",
    "y_teste = dataset_teste['playoff']\n",
    "\n",
    "# Aplicando a transformação para dar mais valores a uns que a outros\n",
    "X_treino = scale_values(X_treino)\n",
    "X_teste = scale_values(X_teste)\n",
    "\n",
    "X_treino.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar ficheiros para guardar resultados e avaliar\n",
    "dataset_resultados = pd.DataFrame({})\n",
    "\n",
    "dataset_resultados[\"playoff\"] = dataset_teste[\"playoff\"]\n",
    "\n",
    "#TODO adicionar a team para depois analisar no final\n",
    "dataset_resultados[\"tmID\"] = dataset_teste[\"tmID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\597684550.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão linear\n",
    "modelo = RandomForestRegressor()\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"RandomForestRegressor\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\450214780.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão linear\n",
    "modelo = GradientBoostingRegressor()\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"GradientBoostingRegressor\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\3123343135.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão linear\n",
    "modelo = LinearRegression()\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"LinearRegression\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\1713031855.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "#::::::::::::::::::::Este modelo é uma variante da regressão linear que usa regularização L2 para reduzir overfitting.::::::::::::::::\n",
    "\n",
    "# Inicializar o modelo de Ridge Regression\n",
    "modelo = Ridge(alpha=1.0)  # alpha controla o nível de regularização\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"Ridge\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\977365062.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Similar ao Ridge, mas utiliza regularização L1. Tende a eliminar variáveis menos importantes, útil para seleção de features.\n",
    "# Inicializar o modelo de Lasso Regression\n",
    "modelo = Lasso(alpha=0.1, random_state=42)  # alpha controla a força da regularização\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\151494901.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Combina as regularizações L1 (Lasso) e L2 (Ridge).\n",
    "# Inicializar o modelo de Elastic Net\n",
    "modelo = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "# `alpha` controla a força total da regularização.\n",
    "# `l1_ratio` controla a proporção de regularização L1 (Lasso) em relação à L2 (Ridge).\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\826227622.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de MLP\n",
    "modelo = MLPRegressor(hidden_layer_sizes=(800, 400),  # Camadas ocultas com 100 e 50 neurônios\n",
    "                      activation='relu',            # Função de ativação\n",
    "                      solver='adam',                # Otimizador\n",
    "                      max_iter=500,                 # Número máximo de iterações\n",
    "                      random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"MLPRegressor_relu\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\3422886681.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de MLP\n",
    "modelo = MLPRegressor(hidden_layer_sizes=(100, 50),  # Camadas ocultas com 100 e 50 neurônios\n",
    "                      activation='tanh',            # Função de ativação\n",
    "                      solver='adam',                # Otimizador\n",
    "                      max_iter=500,                 # Número máximo de iterações\n",
    "                      random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"MLPRegressor_tanh\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\989847951.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de MLP\n",
    "modelo = MLPRegressor(hidden_layer_sizes=(100, 50),  # Camadas ocultas com 100 e 50 neurônios\n",
    "                      activation='logistic',            # Função de ativação\n",
    "                      solver='adam',                # Otimizador\n",
    "                      max_iter=500,                 # Número máximo de iterações\n",
    "                      random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"MLPRegressor_logistic\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - loss: 0.5478 - val_loss: 0.4725\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.3800 - val_loss: 0.3585\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.3122 - val_loss: 0.2860\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2475 - val_loss: 0.2418\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2180 - val_loss: 0.2179\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2131 - val_loss: 0.2124\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2248 - val_loss: 0.2139\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2208 - val_loss: 0.2141\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2256 - val_loss: 0.2128\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2158 - val_loss: 0.2152\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1943 - val_loss: 0.2219\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1977 - val_loss: 0.2350\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2070 - val_loss: 0.2403\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2106 - val_loss: 0.2323\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2027 - val_loss: 0.2205\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1958 - val_loss: 0.2170\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2056 - val_loss: 0.2171\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1968 - val_loss: 0.2171\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2039 - val_loss: 0.2176\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2119 - val_loss: 0.2185\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1851 - val_loss: 0.2187\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2064 - val_loss: 0.2186\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1903 - val_loss: 0.2199\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2002 - val_loss: 0.2215\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1999 - val_loss: 0.2221\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1994 - val_loss: 0.2237\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1889 - val_loss: 0.2251\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1955 - val_loss: 0.2290\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1970 - val_loss: 0.2343\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1929 - val_loss: 0.2353\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1942 - val_loss: 0.2323\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2007 - val_loss: 0.2321\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1974 - val_loss: 0.2301\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1891 - val_loss: 0.2296\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1931 - val_loss: 0.2294\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1939 - val_loss: 0.2341\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1875 - val_loss: 0.2372\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1889 - val_loss: 0.2388\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2042 - val_loss: 0.2410\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1902 - val_loss: 0.2388\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1904 - val_loss: 0.2366\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1847 - val_loss: 0.2359\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1832 - val_loss: 0.2381\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1913 - val_loss: 0.2347\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1927 - val_loss: 0.2320\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1814 - val_loss: 0.2305\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1976 - val_loss: 0.2302\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1966 - val_loss: 0.2301\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1874 - val_loss: 0.2322\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1943 - val_loss: 0.2318\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\2146543663.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inicializar o modelo sequencial\n",
    "modelo = Sequential()\n",
    "\n",
    "# Adicionar camadas\n",
    "modelo.add(Dense(64, activation='relu', input_shape=(X_treino.shape[1],)))  # Primeira camada oculta\n",
    "modelo.add(Dense(32, activation='relu'))  # Segunda camada oculta\n",
    "modelo.add(Dense(1))  # Camada de saída (regressão)\n",
    "\n",
    "# Compilar o modelo\n",
    "modelo.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste).flatten()  # Flatten para transformar em 1D\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"Sequential_relu\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - loss: 0.2650 - val_loss: 0.2492\n",
      "Epoch 2/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2460 - val_loss: 0.2623\n",
      "Epoch 3/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2203 - val_loss: 0.2395\n",
      "Epoch 4/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2088 - val_loss: 0.2836\n",
      "Epoch 5/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2409 - val_loss: 0.2692\n",
      "Epoch 6/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2091 - val_loss: 0.2330\n",
      "Epoch 7/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2057 - val_loss: 0.2418\n",
      "Epoch 8/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.2275 - val_loss: 0.2313\n",
      "Epoch 9/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2170 - val_loss: 0.2316\n",
      "Epoch 10/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2001 - val_loss: 0.2392\n",
      "Epoch 11/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2133 - val_loss: 0.2317\n",
      "Epoch 12/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2044 - val_loss: 0.2271\n",
      "Epoch 13/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1957 - val_loss: 0.2309\n",
      "Epoch 14/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2157 - val_loss: 0.2286\n",
      "Epoch 15/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1995 - val_loss: 0.2367\n",
      "Epoch 16/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2046 - val_loss: 0.2402\n",
      "Epoch 17/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2045 - val_loss: 0.2354\n",
      "Epoch 18/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1988 - val_loss: 0.2321\n",
      "Epoch 19/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2001 - val_loss: 0.2335\n",
      "Epoch 20/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1965 - val_loss: 0.2313\n",
      "Epoch 21/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1964 - val_loss: 0.2338\n",
      "Epoch 22/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1965 - val_loss: 0.2348\n",
      "Epoch 23/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2039 - val_loss: 0.2290\n",
      "Epoch 24/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1954 - val_loss: 0.2303\n",
      "Epoch 25/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2009 - val_loss: 0.2422\n",
      "Epoch 26/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2113 - val_loss: 0.2492\n",
      "Epoch 27/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.2010 - val_loss: 0.2405\n",
      "Epoch 28/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1968 - val_loss: 0.2316\n",
      "Epoch 29/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2053 - val_loss: 0.2317\n",
      "Epoch 30/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1996 - val_loss: 0.2330\n",
      "Epoch 31/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2097 - val_loss: 0.2340\n",
      "Epoch 32/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2039 - val_loss: 0.2329\n",
      "Epoch 33/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2128 - val_loss: 0.2440\n",
      "Epoch 34/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2174 - val_loss: 0.2703\n",
      "Epoch 35/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2076 - val_loss: 0.2580\n",
      "Epoch 36/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.2087 - val_loss: 0.2362\n",
      "Epoch 37/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.2033 - val_loss: 0.2378\n",
      "Epoch 38/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1946 - val_loss: 0.2388\n",
      "Epoch 39/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1983 - val_loss: 0.2470\n",
      "Epoch 40/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1960 - val_loss: 0.2435\n",
      "Epoch 41/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.1997 - val_loss: 0.2367\n",
      "Epoch 42/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.1993 - val_loss: 0.2351\n",
      "Epoch 43/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2008 - val_loss: 0.2412\n",
      "Epoch 44/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.2023 - val_loss: 0.2342\n",
      "Epoch 45/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1990 - val_loss: 0.2299\n",
      "Epoch 46/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1932 - val_loss: 0.2311\n",
      "Epoch 47/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1947 - val_loss: 0.2373\n",
      "Epoch 48/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 0.2041 - val_loss: 0.2373\n",
      "Epoch 49/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 0.1961 - val_loss: 0.2309\n",
      "Epoch 50/50\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.1966 - val_loss: 0.2317\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\2325067666.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inicializar o modelo sequencial\n",
    "modelo = Sequential()\n",
    "\n",
    "# Adicionar camadas\n",
    "modelo.add(Dense(64, activation='tanh', input_shape=(X_treino.shape[1],)))  # Primeira camada oculta\n",
    "modelo.add(Dense(32, activation='tanh'))  # Segunda camada oculta\n",
    "modelo.add(Dense(1))  # Camada de saída (regressão)\n",
    "\n",
    "# Compilar o modelo\n",
    "modelo.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste).flatten()  # Flatten para transformar em 1D\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"Sequential_tanh\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\4200620747.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão com Extra Trees\n",
    "modelo = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"ExtraTreesRegressor\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\2178884686.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo ElasticNet\n",
    "modelo = ElasticNet(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"ElasticNet\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\4215414685.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo XGBoost Regressor\n",
    "modelo = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"XGBRegressor\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000029 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 194\n",
      "[LightGBM] [Info] Number of data points in the train set: 126, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 0.571429\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\191379248.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo LightGBM Regressor\n",
    "modelo = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"LGBMRegressor\"] = y_pred_teste\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previsões corretas: 8 de 12 (66.67%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\2616487240.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n",
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_26472\\2616487240.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['correto'] = ((dataset_teste['playoff'] == 1) & (dataset_teste['playoff_previsto'] > 0.5)) | \\\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão com Gradient Boosting\n",
    "modelo = GradientBoostingRegressor(random_state=42) #TODO porque foi o melhor modelo até agora\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "# Critério de correção\n",
    "# 1. Quando 'playoff' é 1, 'playoff_previsto' deve ser > 0.5\n",
    "# 2. Quando 'playoff' é 0, 'playoff_previsto' deve ser <= 0.5\n",
    "\n",
    "# Criar uma coluna indicando se a previsão está correta\n",
    "dataset_teste['correto'] = ((dataset_teste['playoff'] == 1) & (dataset_teste['playoff_previsto'] > 0.5)) | \\\n",
    "                           ((dataset_teste['playoff'] == 0) & (dataset_teste['playoff_previsto'] <= 0.5))\n",
    "\n",
    "# Contar o número de previsões corretas\n",
    "corretos = dataset_teste['correto'].sum()\n",
    "\n",
    "# Número total de exemplos\n",
    "total = len(dataset_teste)\n",
    "\n",
    "# Exibir o resultado\n",
    "print(f\"Previsões corretas: {corretos} de {total} ({(corretos / total) * 100:.2f}%)\")\n",
    "\n",
    "dataset_resultados[\"GradientBoostingRegressor\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playoff</th>\n",
       "      <th>tmID</th>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>MLPRegressor_relu</th>\n",
       "      <th>MLPRegressor_tanh</th>\n",
       "      <th>MLPRegressor_logistic</th>\n",
       "      <th>Sequential_relu</th>\n",
       "      <th>Sequential_tanh</th>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <th>ElasticNet</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.775521</td>\n",
       "      <td>0.740620</td>\n",
       "      <td>0.727164</td>\n",
       "      <td>0.966151</td>\n",
       "      <td>0.802379</td>\n",
       "      <td>0.685762</td>\n",
       "      <td>0.783768</td>\n",
       "      <td>0.810381</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.979474</td>\n",
       "      <td>0.911868</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.359892</td>\n",
       "      <td>0.196110</td>\n",
       "      <td>0.230825</td>\n",
       "      <td>0.483791</td>\n",
       "      <td>0.188222</td>\n",
       "      <td>0.678814</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.105498</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.075729</td>\n",
       "      <td>0.270132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.0</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.737859</td>\n",
       "      <td>0.441378</td>\n",
       "      <td>0.453195</td>\n",
       "      <td>1.282022</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.683489</td>\n",
       "      <td>0.556051</td>\n",
       "      <td>0.551604</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.878499</td>\n",
       "      <td>0.458302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.823855</td>\n",
       "      <td>0.826264</td>\n",
       "      <td>0.799007</td>\n",
       "      <td>1.479369</td>\n",
       "      <td>0.861123</td>\n",
       "      <td>0.685902</td>\n",
       "      <td>0.752210</td>\n",
       "      <td>0.875342</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.909940</td>\n",
       "      <td>0.819967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.0</td>\n",
       "      <td>LAS</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.793922</td>\n",
       "      <td>0.730218</td>\n",
       "      <td>0.718469</td>\n",
       "      <td>0.804639</td>\n",
       "      <td>0.787387</td>\n",
       "      <td>0.685404</td>\n",
       "      <td>0.770687</td>\n",
       "      <td>0.789009</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.910552</td>\n",
       "      <td>0.884533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.417061</td>\n",
       "      <td>0.824053</td>\n",
       "      <td>0.815782</td>\n",
       "      <td>0.030703</td>\n",
       "      <td>0.871779</td>\n",
       "      <td>0.686033</td>\n",
       "      <td>0.789021</td>\n",
       "      <td>0.816825</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.316502</td>\n",
       "      <td>0.581105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NYL</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.231559</td>\n",
       "      <td>0.359874</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>-0.083225</td>\n",
       "      <td>0.401832</td>\n",
       "      <td>0.681327</td>\n",
       "      <td>0.348835</td>\n",
       "      <td>0.355040</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.042231</td>\n",
       "      <td>0.285239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1.0</td>\n",
       "      <td>PHO</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.884244</td>\n",
       "      <td>0.660040</td>\n",
       "      <td>0.654003</td>\n",
       "      <td>0.437732</td>\n",
       "      <td>0.724538</td>\n",
       "      <td>0.685054</td>\n",
       "      <td>0.590684</td>\n",
       "      <td>0.667382</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.771770</td>\n",
       "      <td>0.696829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.285054</td>\n",
       "      <td>1.042231</td>\n",
       "      <td>1.004143</td>\n",
       "      <td>-0.690004</td>\n",
       "      <td>1.083038</td>\n",
       "      <td>0.688881</td>\n",
       "      <td>0.913036</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.423804</td>\n",
       "      <td>0.428496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SEA</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.603229</td>\n",
       "      <td>0.840104</td>\n",
       "      <td>0.821915</td>\n",
       "      <td>1.217713</td>\n",
       "      <td>0.898806</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.843510</td>\n",
       "      <td>0.952336</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.727626</td>\n",
       "      <td>0.744822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>0.0</td>\n",
       "      <td>TUL</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.033848</td>\n",
       "      <td>0.120438</td>\n",
       "      <td>0.154214</td>\n",
       "      <td>-0.168994</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.678228</td>\n",
       "      <td>0.077082</td>\n",
       "      <td>0.076441</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.323809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>1.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.466098</td>\n",
       "      <td>0.678490</td>\n",
       "      <td>0.674698</td>\n",
       "      <td>0.132196</td>\n",
       "      <td>0.731421</td>\n",
       "      <td>0.684246</td>\n",
       "      <td>0.731512</td>\n",
       "      <td>0.686037</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.324425</td>\n",
       "      <td>0.590485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     playoff tmID  RandomForestRegressor  GradientBoostingRegressor  \\\n",
       "2        1.0  ATL                   0.88                   0.775521   \n",
       "13       0.0  CHI                   0.14                   0.359892   \n",
       "24       1.0  CON                   0.58                   0.737859   \n",
       "51       1.0  IND                   0.79                   0.823855   \n",
       "61       0.0  LAS                   0.85                   0.793922   \n",
       "73       0.0  MIN                   0.48                   0.417061   \n",
       "83       1.0  NYL                   0.06                  -0.231559   \n",
       "95       1.0  PHO                   0.73                   0.884244   \n",
       "114      1.0  SAS                   0.46                   0.285054   \n",
       "124      1.0  SEA                   0.76                   0.603229   \n",
       "125      0.0  TUL                   0.09                   0.033848   \n",
       "137      1.0  WAS                   0.50                   0.466098   \n",
       "\n",
       "     LinearRegression     Ridge  MLPRegressor_relu  MLPRegressor_tanh  \\\n",
       "2            0.740620  0.727164           0.966151           0.802379   \n",
       "13           0.196110  0.230825           0.483791           0.188222   \n",
       "24           0.441378  0.453195           1.282022           0.510145   \n",
       "51           0.826264  0.799007           1.479369           0.861123   \n",
       "61           0.730218  0.718469           0.804639           0.787387   \n",
       "73           0.824053  0.815782           0.030703           0.871779   \n",
       "83           0.359874  0.379743          -0.083225           0.401832   \n",
       "95           0.660040  0.654003           0.437732           0.724538   \n",
       "114          1.042231  1.004143          -0.690004           1.083038   \n",
       "124          0.840104  0.821915           1.217713           0.898806   \n",
       "125          0.120438  0.154214          -0.168994           0.109000   \n",
       "137          0.678490  0.674698           0.132196           0.731421   \n",
       "\n",
       "     MLPRegressor_logistic  Sequential_relu  Sequential_tanh  \\\n",
       "2                 0.685762         0.783768         0.810381   \n",
       "13                0.678814         0.141179         0.105498   \n",
       "24                0.683489         0.556051         0.551604   \n",
       "51                0.685902         0.752210         0.875342   \n",
       "61                0.685404         0.770687         0.789009   \n",
       "73                0.686033         0.789021         0.816825   \n",
       "83                0.681327         0.348835         0.355040   \n",
       "95                0.685054         0.590684         0.667382   \n",
       "114               0.688881         0.913036         1.136364   \n",
       "124               0.687831         0.843510         0.952336   \n",
       "125               0.678228         0.077082         0.076441   \n",
       "137               0.684246         0.731512         0.686037   \n",
       "\n",
       "     ExtraTreesRegressor  ElasticNet  XGBRegressor  LGBMRegressor  \n",
       "2                   0.74    0.571429      0.979474       0.911868  \n",
       "13                  0.04    0.571429      0.075729       0.270132  \n",
       "24                  0.61    0.571429      0.878499       0.458302  \n",
       "51                  0.78    0.571429      0.909940       0.819967  \n",
       "61                  0.68    0.571429      0.910552       0.884533  \n",
       "73                  0.63    0.571429      0.316502       0.581105  \n",
       "83                  0.10    0.571429     -0.042231       0.285239  \n",
       "95                  0.75    0.571429      0.771770       0.696829  \n",
       "114                 0.57    0.571429      0.423804       0.428496  \n",
       "124                 0.75    0.571429      0.727626       0.744822  \n",
       "125                 0.00    0.571429     -0.008929       0.323809  \n",
       "137                 0.57    0.571429      0.324425       0.590485  "
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Para mostrar os resultados obtidos pelos varios modelos\n",
    "dataset_resultados.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playoff</th>\n",
       "      <th>tmID</th>\n",
       "      <th>RandomForestRegressor</th>\n",
       "      <th>GradientBoostingRegressor</th>\n",
       "      <th>LinearRegression</th>\n",
       "      <th>Ridge</th>\n",
       "      <th>MLPRegressor_relu</th>\n",
       "      <th>MLPRegressor_tanh</th>\n",
       "      <th>MLPRegressor_logistic</th>\n",
       "      <th>Sequential_relu</th>\n",
       "      <th>Sequential_tanh</th>\n",
       "      <th>ExtraTreesRegressor</th>\n",
       "      <th>ElasticNet</th>\n",
       "      <th>XGBRegressor</th>\n",
       "      <th>LGBMRegressor</th>\n",
       "      <th>confID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0.88</td>\n",
       "      <td>0.775521</td>\n",
       "      <td>0.740620</td>\n",
       "      <td>0.727164</td>\n",
       "      <td>0.966151</td>\n",
       "      <td>0.802379</td>\n",
       "      <td>0.685762</td>\n",
       "      <td>0.783768</td>\n",
       "      <td>0.810381</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.979474</td>\n",
       "      <td>0.911868</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.359892</td>\n",
       "      <td>0.196110</td>\n",
       "      <td>0.230825</td>\n",
       "      <td>0.483791</td>\n",
       "      <td>0.188222</td>\n",
       "      <td>0.678814</td>\n",
       "      <td>0.141179</td>\n",
       "      <td>0.105498</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.075729</td>\n",
       "      <td>0.270132</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.737859</td>\n",
       "      <td>0.441378</td>\n",
       "      <td>0.453195</td>\n",
       "      <td>1.282022</td>\n",
       "      <td>0.510145</td>\n",
       "      <td>0.683489</td>\n",
       "      <td>0.556051</td>\n",
       "      <td>0.551604</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.878499</td>\n",
       "      <td>0.458302</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.823855</td>\n",
       "      <td>0.826264</td>\n",
       "      <td>0.799007</td>\n",
       "      <td>1.479369</td>\n",
       "      <td>0.861123</td>\n",
       "      <td>0.685902</td>\n",
       "      <td>0.752210</td>\n",
       "      <td>0.875342</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.909940</td>\n",
       "      <td>0.819967</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>LAS</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.793922</td>\n",
       "      <td>0.730218</td>\n",
       "      <td>0.718469</td>\n",
       "      <td>0.804639</td>\n",
       "      <td>0.787387</td>\n",
       "      <td>0.685404</td>\n",
       "      <td>0.770687</td>\n",
       "      <td>0.789009</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.910552</td>\n",
       "      <td>0.884533</td>\n",
       "      <td>WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.417061</td>\n",
       "      <td>0.824053</td>\n",
       "      <td>0.815782</td>\n",
       "      <td>0.030703</td>\n",
       "      <td>0.871779</td>\n",
       "      <td>0.686033</td>\n",
       "      <td>0.789021</td>\n",
       "      <td>0.816825</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.316502</td>\n",
       "      <td>0.581105</td>\n",
       "      <td>WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NYL</td>\n",
       "      <td>0.06</td>\n",
       "      <td>-0.231559</td>\n",
       "      <td>0.359874</td>\n",
       "      <td>0.379743</td>\n",
       "      <td>-0.083225</td>\n",
       "      <td>0.401832</td>\n",
       "      <td>0.681327</td>\n",
       "      <td>0.348835</td>\n",
       "      <td>0.355040</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.042231</td>\n",
       "      <td>0.285239</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>PHO</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.884244</td>\n",
       "      <td>0.660040</td>\n",
       "      <td>0.654003</td>\n",
       "      <td>0.437732</td>\n",
       "      <td>0.724538</td>\n",
       "      <td>0.685054</td>\n",
       "      <td>0.590684</td>\n",
       "      <td>0.667382</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.771770</td>\n",
       "      <td>0.696829</td>\n",
       "      <td>WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.285054</td>\n",
       "      <td>1.042231</td>\n",
       "      <td>1.004143</td>\n",
       "      <td>-0.690004</td>\n",
       "      <td>1.083038</td>\n",
       "      <td>0.688881</td>\n",
       "      <td>0.913036</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.423804</td>\n",
       "      <td>0.428496</td>\n",
       "      <td>WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SEA</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.603229</td>\n",
       "      <td>0.840104</td>\n",
       "      <td>0.821915</td>\n",
       "      <td>1.217713</td>\n",
       "      <td>0.898806</td>\n",
       "      <td>0.687831</td>\n",
       "      <td>0.843510</td>\n",
       "      <td>0.952336</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.727626</td>\n",
       "      <td>0.744822</td>\n",
       "      <td>WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>TUL</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.033848</td>\n",
       "      <td>0.120438</td>\n",
       "      <td>0.154214</td>\n",
       "      <td>-0.168994</td>\n",
       "      <td>0.109000</td>\n",
       "      <td>0.678228</td>\n",
       "      <td>0.077082</td>\n",
       "      <td>0.076441</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>-0.008929</td>\n",
       "      <td>0.323809</td>\n",
       "      <td>WE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.466098</td>\n",
       "      <td>0.678490</td>\n",
       "      <td>0.674698</td>\n",
       "      <td>0.132196</td>\n",
       "      <td>0.731421</td>\n",
       "      <td>0.684246</td>\n",
       "      <td>0.731512</td>\n",
       "      <td>0.686037</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.324425</td>\n",
       "      <td>0.590485</td>\n",
       "      <td>EA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    playoff tmID  RandomForestRegressor  GradientBoostingRegressor  \\\n",
       "0       1.0  ATL                   0.88                   0.775521   \n",
       "1       0.0  CHI                   0.14                   0.359892   \n",
       "2       1.0  CON                   0.58                   0.737859   \n",
       "3       1.0  IND                   0.79                   0.823855   \n",
       "4       0.0  LAS                   0.85                   0.793922   \n",
       "5       0.0  MIN                   0.48                   0.417061   \n",
       "6       1.0  NYL                   0.06                  -0.231559   \n",
       "7       1.0  PHO                   0.73                   0.884244   \n",
       "8       1.0  SAS                   0.46                   0.285054   \n",
       "9       1.0  SEA                   0.76                   0.603229   \n",
       "10      0.0  TUL                   0.09                   0.033848   \n",
       "11      1.0  WAS                   0.50                   0.466098   \n",
       "\n",
       "    LinearRegression     Ridge  MLPRegressor_relu  MLPRegressor_tanh  \\\n",
       "0           0.740620  0.727164           0.966151           0.802379   \n",
       "1           0.196110  0.230825           0.483791           0.188222   \n",
       "2           0.441378  0.453195           1.282022           0.510145   \n",
       "3           0.826264  0.799007           1.479369           0.861123   \n",
       "4           0.730218  0.718469           0.804639           0.787387   \n",
       "5           0.824053  0.815782           0.030703           0.871779   \n",
       "6           0.359874  0.379743          -0.083225           0.401832   \n",
       "7           0.660040  0.654003           0.437732           0.724538   \n",
       "8           1.042231  1.004143          -0.690004           1.083038   \n",
       "9           0.840104  0.821915           1.217713           0.898806   \n",
       "10          0.120438  0.154214          -0.168994           0.109000   \n",
       "11          0.678490  0.674698           0.132196           0.731421   \n",
       "\n",
       "    MLPRegressor_logistic  Sequential_relu  Sequential_tanh  \\\n",
       "0                0.685762         0.783768         0.810381   \n",
       "1                0.678814         0.141179         0.105498   \n",
       "2                0.683489         0.556051         0.551604   \n",
       "3                0.685902         0.752210         0.875342   \n",
       "4                0.685404         0.770687         0.789009   \n",
       "5                0.686033         0.789021         0.816825   \n",
       "6                0.681327         0.348835         0.355040   \n",
       "7                0.685054         0.590684         0.667382   \n",
       "8                0.688881         0.913036         1.136364   \n",
       "9                0.687831         0.843510         0.952336   \n",
       "10               0.678228         0.077082         0.076441   \n",
       "11               0.684246         0.731512         0.686037   \n",
       "\n",
       "    ExtraTreesRegressor  ElasticNet  XGBRegressor  LGBMRegressor confID  \n",
       "0                  0.74    0.571429      0.979474       0.911868     EA  \n",
       "1                  0.04    0.571429      0.075729       0.270132     EA  \n",
       "2                  0.61    0.571429      0.878499       0.458302     EA  \n",
       "3                  0.78    0.571429      0.909940       0.819967     EA  \n",
       "4                  0.68    0.571429      0.910552       0.884533     WE  \n",
       "5                  0.63    0.571429      0.316502       0.581105     WE  \n",
       "6                  0.10    0.571429     -0.042231       0.285239     EA  \n",
       "7                  0.75    0.571429      0.771770       0.696829     WE  \n",
       "8                  0.57    0.571429      0.423804       0.428496     WE  \n",
       "9                  0.75    0.571429      0.727626       0.744822     WE  \n",
       "10                 0.00    0.571429     -0.008929       0.323809     WE  \n",
       "11                 0.57    0.571429      0.324425       0.590485     EA  "
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO adicionar as confID para selecionar os 4 melhores de cada liga\n",
    "\n",
    "#Seleciona os valores para cada equipa da conf\n",
    "dataset_para_obter_confID = pd.read_csv('cleanDatasets/team_year_stats.csv')\n",
    "y_unique = dataset_para_obter_confID[['tmID', 'confID']].drop_duplicates()\n",
    "\n",
    "# Adiciona a confID ao dataset x com base na correspondência com y\n",
    "dataset_resultados = dataset_resultados.merge(y_unique[['tmID', 'confID']], on='tmID', how='left')\n",
    "\n",
    "dataset_resultados.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R²</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.297401</td>\n",
       "      <td>0.438368</td>\n",
       "      <td>0.338306</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.194235</td>\n",
       "      <td>0.363857</td>\n",
       "      <td>0.125942</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.192882</td>\n",
       "      <td>0.367809</td>\n",
       "      <td>0.132032</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPRegressor_relu</td>\n",
       "      <td>0.530633</td>\n",
       "      <td>0.558698</td>\n",
       "      <td>1.387848</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPRegressor_tanh</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>0.342432</td>\n",
       "      <td>0.156798</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPRegressor_logistic</td>\n",
       "      <td>0.221123</td>\n",
       "      <td>0.437166</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sequential_relu</td>\n",
       "      <td>0.186944</td>\n",
       "      <td>0.354864</td>\n",
       "      <td>0.158752</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sequential_tanh</td>\n",
       "      <td>0.183776</td>\n",
       "      <td>0.335501</td>\n",
       "      <td>0.173007</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>0.202817</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>0.087325</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.750</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Modelo       MSE       MAE        R²  Precision  Recall  \\\n",
       "0      RandomForestRegressor  0.230933  0.400000  0.039200      0.750   0.750   \n",
       "1  GradientBoostingRegressor  0.297401  0.438368  0.338306      0.750   0.750   \n",
       "2           LinearRegression  0.194235  0.363857  0.125942      0.750   0.750   \n",
       "3                      Ridge  0.192882  0.367809  0.132032      0.750   0.750   \n",
       "4          MLPRegressor_relu  0.530633  0.558698  1.387848      0.625   0.625   \n",
       "5          MLPRegressor_tanh  0.187378  0.342432  0.156798      0.750   0.750   \n",
       "6      MLPRegressor_logistic  0.221123  0.437166  0.004948      0.750   0.750   \n",
       "7            Sequential_relu  0.186944  0.354864  0.158752      0.750   0.750   \n",
       "8            Sequential_tanh  0.183776  0.335501  0.173007      0.750   0.750   \n",
       "9        ExtraTreesRegressor  0.202817  0.373333  0.087325      0.750   0.750   \n",
       "\n",
       "   Accuracy  \n",
       "0  0.666667  \n",
       "1  0.666667  \n",
       "2  0.666667  \n",
       "3  0.666667  \n",
       "4  0.500000  \n",
       "5  0.666667  \n",
       "6  0.666667  \n",
       "7  0.666667  \n",
       "8  0.666667  \n",
       "9  0.666667  "
      ]
     },
     "execution_count": 600,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame(dataset_resultados)\n",
    "\n",
    "# Inicializar lista para armazenar os resultados\n",
    "resultados = []\n",
    "\n",
    "# Número de melhores previsões para considerar como 1 por conferência\n",
    "top_n_conferencia = 4\n",
    "\n",
    "# Loop sobre cada coluna do modelo (excluindo a coluna 'playoff')\n",
    "for modelo in df.columns[2:-1]:\n",
    "    # Cálculo de métricas de regressão\n",
    "    mse = mean_squared_error(df[\"playoff\"], df[modelo])\n",
    "    mae = mean_absolute_error(df[\"playoff\"], df[modelo])\n",
    "    r2 = abs(r2_score(df[\"playoff\"], df[modelo]))\n",
    "    \n",
    "    # Binarização dos valores (top 4 melhores de cada conferência como 1)\n",
    "    y_true = (df[\"playoff\"] >= 0.5).astype(int)\n",
    "    y_pred = pd.Series(0, index=df.index)  # Inicializa com 0 para todos\n",
    "    \n",
    "    # Separa os dados por conferência\n",
    "    conferencia_1 = df[df[\"confID\"] == \"WE\"]\n",
    "    conferencia_2 = df[df[\"confID\"] == \"EA\"]\n",
    "    \n",
    "    # Obtém os índices dos top 4 de cada conferência\n",
    "    top_indices_1 = conferencia_1[modelo].nlargest(top_n_conferencia).index\n",
    "    top_indices_2 = conferencia_2[modelo].nlargest(top_n_conferencia).index\n",
    "    \n",
    "    # Define 1 para os top 4 de cada conferência\n",
    "    y_pred.loc[top_indices_1] = 1\n",
    "    y_pred.loc[top_indices_2] = 1\n",
    "    \n",
    "    # Cálculo de métricas de classificação\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Adicionar os resultados à lista\n",
    "    resultados.append({\n",
    "        \"Modelo\": modelo,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "resultados_4_melhores = pd.DataFrame(resultados)\n",
    "\n",
    "# Exibir os resultados\n",
    "resultados_4_melhores.head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Criar o DataFrame com os dados necessários\\nresultados_4_melhores = pd.DataFrame({\\n    \"playoff\": dataset_resultados[\"playoff\"],\\n    \"prev_regression\": dataset_resultados[\"GradientBoostingRegressor\"]\\n})\\n\\n# Ordenar o DataFrame pela coluna \\'prev_regression\\' de forma decrescente\\nresultados_4_melhores_sorted = resultados_4_melhores.sort_values(\\'prev_regression\\', ascending=False)\\n\\n# Criar a nova coluna \\'top_8\\' que marca as 8 melhores entradas como 1 e as outras como 0\\nresultados_4_melhores_sorted[\\'top_8\\'] = 0\\n\\n# Marcar as 8 melhores previsões como 1\\nresultados_4_melhores_sorted.iloc[:8, resultados_4_melhores_sorted.columns.get_loc(\\'top_8\\')] = 1\\n\\n# Reordenar o DataFrame para a ordem original\\nresultados_4_melhores_final = resultados_4_melhores_sorted.sort_index()\\n\\n# Assumindo que \\'top_8\\' seja a variável de verdade (y_true)\\ny_true = resultados_4_melhores_final[\\'top_8\\']\\n\\n# Encontrar o limiar de corte para as 8 melhores entradas em \\'prev_regression\\'\\nthreshold = resultados_4_melhores_final[\\'prev_regression\\'].nlargest(8).min()\\n\\n# Converter as previsões de \\'prev_regression\\' em binário, onde 1 significa estar entre as 8 melhores previsões\\ny_pred = (resultados_4_melhores_final[\\'prev_regression\\'] >= threshold).astype(int)\\n\\n# Para mudar o nome de uma coluna, basta usar o método rename\\nresultados_4_melhores_final = resultados_4_melhores_final.rename(columns={\\'top_8\\': \\'prev_class\\'})\\n\\n#......................Desenhar a curva.................................\\nfrom sklearn.metrics import precision_recall_curve\\n\\n# Supondo que \\'resultados_4_melhores_final\\' tenha as colunas \\'playoff\\' e \\'prev_class\\'\\ny_true = resultados_4_melhores_final[\\'playoff\\']  # Rótulos reais\\ny_scores = resultados_4_melhores_final[\\'prev_class\\']  # Probabilidades ou escores de previsão\\n\\n# Calcular a curva Precision-Recall\\nprecision, recall, thresholds = precision_recall_curve(y_true, y_scores)\\n\\n# Plotar a curva Precision-Recall\\nplt.figure(figsize=(8, 6))\\nplt.plot(recall, precision, marker=\\'.\\', label=\\'Curva Precision-Recall\\')\\nplt.xlabel(\\'Recall\\')\\nplt.ylabel(\\'Precision\\')\\nplt.title(\\'Curva Precision-Recall, classifica as 8 melhores equipas com 1 e as outras com 0\\')\\nplt.grid(True)\\nplt.legend()\\nplt.show()\\n\\nresultados_4_melhores_final.head(20)\\n'"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Criar o DataFrame com os dados necessários\n",
    "resultados_4_melhores = pd.DataFrame({\n",
    "    \"playoff\": dataset_resultados[\"playoff\"],\n",
    "    \"prev_regression\": dataset_resultados[\"GradientBoostingRegressor\"]\n",
    "})\n",
    "\n",
    "# Ordenar o DataFrame pela coluna 'prev_regression' de forma decrescente\n",
    "resultados_4_melhores_sorted = resultados_4_melhores.sort_values('prev_regression', ascending=False)\n",
    "\n",
    "# Criar a nova coluna 'top_8' que marca as 8 melhores entradas como 1 e as outras como 0\n",
    "resultados_4_melhores_sorted['top_8'] = 0\n",
    "\n",
    "# Marcar as 8 melhores previsões como 1\n",
    "resultados_4_melhores_sorted.iloc[:8, resultados_4_melhores_sorted.columns.get_loc('top_8')] = 1\n",
    "\n",
    "# Reordenar o DataFrame para a ordem original\n",
    "resultados_4_melhores_final = resultados_4_melhores_sorted.sort_index()\n",
    "\n",
    "# Assumindo que 'top_8' seja a variável de verdade (y_true)\n",
    "y_true = resultados_4_melhores_final['top_8']\n",
    "\n",
    "# Encontrar o limiar de corte para as 8 melhores entradas em 'prev_regression'\n",
    "threshold = resultados_4_melhores_final['prev_regression'].nlargest(8).min()\n",
    "\n",
    "# Converter as previsões de 'prev_regression' em binário, onde 1 significa estar entre as 8 melhores previsões\n",
    "y_pred = (resultados_4_melhores_final['prev_regression'] >= threshold).astype(int)\n",
    "\n",
    "# Para mudar o nome de uma coluna, basta usar o método rename\n",
    "resultados_4_melhores_final = resultados_4_melhores_final.rename(columns={'top_8': 'prev_class'})\n",
    "\n",
    "#......................Desenhar a curva.................................\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Supondo que 'resultados_4_melhores_final' tenha as colunas 'playoff' e 'prev_class'\n",
    "y_true = resultados_4_melhores_final['playoff']  # Rótulos reais\n",
    "y_scores = resultados_4_melhores_final['prev_class']  # Probabilidades ou escores de previsão\n",
    "\n",
    "# Calcular a curva Precision-Recall\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "# Plotar a curva Precision-Recall\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(recall, precision, marker='.', label='Curva Precision-Recall')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Curva Precision-Recall, classifica as 8 melhores equipas com 1 e as outras com 0')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "resultados_4_melhores_final.head(20)\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Modelo</th>\n",
       "      <th>MSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R²</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>0.230933</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.039200</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>0.297401</td>\n",
       "      <td>0.438368</td>\n",
       "      <td>0.338306</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>0.194235</td>\n",
       "      <td>0.363857</td>\n",
       "      <td>0.125942</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.192882</td>\n",
       "      <td>0.367809</td>\n",
       "      <td>0.132032</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPRegressor_relu</td>\n",
       "      <td>0.530633</td>\n",
       "      <td>0.558698</td>\n",
       "      <td>1.387848</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPRegressor_tanh</td>\n",
       "      <td>0.187378</td>\n",
       "      <td>0.342432</td>\n",
       "      <td>0.156798</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPRegressor_logistic</td>\n",
       "      <td>0.221123</td>\n",
       "      <td>0.437166</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sequential_relu</td>\n",
       "      <td>0.186944</td>\n",
       "      <td>0.354864</td>\n",
       "      <td>0.158752</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sequential_tanh</td>\n",
       "      <td>0.183776</td>\n",
       "      <td>0.335501</td>\n",
       "      <td>0.173007</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>0.202817</td>\n",
       "      <td>0.373333</td>\n",
       "      <td>0.087325</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Modelo       MSE       MAE        R²  Precision  Recall  \\\n",
       "0      RandomForestRegressor  0.230933  0.400000  0.039200       0.75    0.75   \n",
       "1  GradientBoostingRegressor  0.297401  0.438368  0.338306       0.75    0.75   \n",
       "2           LinearRegression  0.194235  0.363857  0.125942       0.75    0.75   \n",
       "3                      Ridge  0.192882  0.367809  0.132032       0.75    0.75   \n",
       "4          MLPRegressor_relu  0.530633  0.558698  1.387848       0.75    0.75   \n",
       "5          MLPRegressor_tanh  0.187378  0.342432  0.156798       0.75    0.75   \n",
       "6      MLPRegressor_logistic  0.221123  0.437166  0.004948       0.75    0.75   \n",
       "7            Sequential_relu  0.186944  0.354864  0.158752       0.75    0.75   \n",
       "8            Sequential_tanh  0.183776  0.335501  0.173007       0.75    0.75   \n",
       "9        ExtraTreesRegressor  0.202817  0.373333  0.087325       0.75    0.75   \n",
       "\n",
       "   Accuracy  \n",
       "0  0.666667  \n",
       "1  0.666667  \n",
       "2  0.666667  \n",
       "3  0.666667  \n",
       "4  0.666667  \n",
       "5  0.666667  \n",
       "6  0.666667  \n",
       "7  0.666667  \n",
       "8  0.666667  \n",
       "9  0.666667  "
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Com os 8 melhores\n",
    "# Resultados considerando que ficam a 1 os 8 melhores\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame(dataset_resultados)\n",
    "\n",
    "# Inicializar lista para armazenar os resultados\n",
    "resultados = []\n",
    "\n",
    "# Número de melhores previsões para considerar como 1\n",
    "top_n = 8\n",
    "\n",
    "# Loop sobre cada coluna do modelo (excluindo a coluna 'playoff')\n",
    "for modelo in df.columns[2:-1]:\n",
    "    # Cálculo de métricas de regressão\n",
    "    mse = mean_squared_error(df[\"playoff\"], df[modelo])\n",
    "    mae = mean_absolute_error(df[\"playoff\"], df[modelo])\n",
    "    r2 = abs(r2_score(df[\"playoff\"], df[modelo]))\n",
    "    \n",
    "    # Binarização dos valores (top 8 melhores como 1)\n",
    "    y_true = (df[\"playoff\"] >= 0.5).astype(int)\n",
    "    y_pred = pd.Series(0, index=df.index)  # Inicializa com 0 para todos\n",
    "    top_indices = df[modelo].nlargest(top_n).index  # Obtém índices dos top 8 valores\n",
    "    y_pred.loc[top_indices] = 1  # Define 1 para os top 8 valores\n",
    "    \n",
    "    # Cálculo de métricas de classificação\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Adicionar os resultados à lista\n",
    "    resultados.append({\n",
    "        \"Modelo\": modelo,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "resultados_8_melhores = pd.DataFrame(resultados)\n",
    "\n",
    "# Exibir os resultados\n",
    "resultados_8_melhores.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro total: 3.49\n",
      "Adj total: 7.970000000000001\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Final Error</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RandomForestRegressor</td>\n",
       "      <td>3.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoostingRegressor</td>\n",
       "      <td>4.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>3.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>3.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLPRegressor_relu</td>\n",
       "      <td>4.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLPRegressor_tanh</td>\n",
       "      <td>3.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLPRegressor_logistic</td>\n",
       "      <td>3.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sequential_relu</td>\n",
       "      <td>3.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sequential_tanh</td>\n",
       "      <td>3.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ExtraTreesRegressor</td>\n",
       "      <td>3.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ElasticNet</td>\n",
       "      <td>5.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LGBMRegressor</td>\n",
       "      <td>4.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Model  Final Error\n",
       "0       RandomForestRegressor         3.87\n",
       "1   GradientBoostingRegressor         4.66\n",
       "2            LinearRegression         3.90\n",
       "3                       Ridge         3.93\n",
       "4           MLPRegressor_relu         4.82\n",
       "5           MLPRegressor_tanh         3.82\n",
       "6       MLPRegressor_logistic         3.69\n",
       "7             Sequential_relu         3.73\n",
       "8             Sequential_tanh         3.68\n",
       "9         ExtraTreesRegressor         3.49\n",
       "10                 ElasticNet         5.32\n",
       "11               XGBRegressor         3.74\n",
       "12              LGBMRegressor         4.42"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO Avaliar da forma do stor\n",
    "\n",
    "# Identificar as colunas dos modelos (exceto playoff, tmID e confID)\n",
    "model_columns = [\n",
    "    col for col in dataset_resultados.columns \n",
    "    if col not in ['playoff', 'tmID', 'confID'] and not col.startswith(('adj_', 'error_'))\n",
    "]\n",
    "\n",
    "# Aplicar a transformação para cada coluna de modelo\n",
    "for col in model_columns:\n",
    "    # Calcular o valor ajustado (adj. pred)\n",
    "    dataset_resultados[f'adj_{col}'] = 8 * dataset_resultados[col] / dataset_resultados[col].sum()\n",
    "    \n",
    "    # Normalizar os valores para estarem entre 0 e 1\n",
    "    min_val = dataset_resultados[f'adj_{col}'].min()\n",
    "    max_val = dataset_resultados[f'adj_{col}'].max()\n",
    "    if max_val - min_val != 0:\n",
    "        dataset_resultados[f'adj_{col}'] = (dataset_resultados[f'adj_{col}'] - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Arredondar os valores para duas casas decimais\n",
    "    dataset_resultados[f'adj_{col}'] = dataset_resultados[f'adj_{col}'].round(2)\n",
    "    \n",
    "    # Ajustar os valores para garantir que a soma seja 8\n",
    "    total_sum = dataset_resultados[f'adj_{col}'].sum()\n",
    "    if total_sum != 0:\n",
    "        adjustment_factor = 8 / total_sum\n",
    "        dataset_resultados[f'adj_{col}'] = (dataset_resultados[f'adj_{col}'] * adjustment_factor).round(2)\n",
    "    \n",
    "    # Garantir que os valores estejam entre 0 e 1 após o ajuste\n",
    "    dataset_resultados[f'adj_{col}'] = dataset_resultados[f'adj_{col}'].clip(0, 1)\n",
    "    \n",
    "    # Calcular o erro absoluto em relação a 'playoff'\n",
    "    dataset_resultados[f'error_{col}'] = abs(dataset_resultados[f'adj_{col}'] - dataset_resultados['playoff'])\n",
    "\n",
    "# Identificar as colunas de erro (que começam com 'error_')\n",
    "error_columns = [col for col in dataset_resultados.columns if col.startswith('error_')]\n",
    "\n",
    "# Criar um dicionário com os erros finais para cada modelo\n",
    "final_errors = {\n",
    "    col.replace('error_', ''): dataset_resultados[col].sum() for col in error_columns\n",
    "}\n",
    "\n",
    "# Criar um DataFrame com os resultados finais\n",
    "error_summary = pd.DataFrame(final_errors.items(), columns=['Model', 'Final Error'])\n",
    "\n",
    "# Salvar o resultado num CSV\n",
    "output_columns = dataset_resultados[['tmID', 'adj_ExtraTreesRegressor']]\n",
    "output_columns = output_columns.rename(columns={'adj_ExtraTreesRegressor': 'Playoff'})\n",
    "output_columns.to_csv('results4.csv', index=False)\n",
    "\n",
    "teste = dataset_resultados[['playoff','tmID','adj_ExtraTreesRegressor','error_ExtraTreesRegressor']]\n",
    "\n",
    "print(\"Erro total:\", teste['error_ExtraTreesRegressor'].sum())\n",
    "print(\"Adj total:\", teste['adj_ExtraTreesRegressor'].sum())\n",
    "error_summary.head(15)\n",
    "#teste.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
