{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df1 = pd.read_csv('datasets/players.csv')\n",
    "df2 = pd.read_csv('datasets/players_teams.csv')\n",
    "df3 = pd.read_csv('datasets/awards_players.csv')\n",
    "df4 = pd.read_csv('datasets/teams.csv')\n",
    "df5 = pd.read_csv('datasets/teams_post.csv')\n",
    "df6 = pd.read_csv('datasets/coaches.csv')\n",
    "df7 = pd.read_csv('datasets/series_post.csv')\n",
    "\n",
    "def corrige_vencedor(teams, series_post):\n",
    "    # Itera sobre cada rodada ('F', 'CF', 'FR') para ajustar cada fase dos playoffs\n",
    "    for round_type in ['FR', 'CF', 'F']:\n",
    "        # Filtra a série específica da rodada\n",
    "        series_round = series_post[series_post['round'] == round_type]\n",
    "        \n",
    "        # Atualiza cada série individualmente\n",
    "        for _, row in series_round.iterrows():\n",
    "            year = row['year']\n",
    "            winner_id = row['tmIDWinner']\n",
    "            loser_id = row['tmIDLoser']\n",
    "            \n",
    "            # Define as colunas que correspondem às rodadas\n",
    "            if round_type == 'FR':\n",
    "                round_column = 'firstRound'\n",
    "            elif round_type == 'CF':\n",
    "                round_column = 'semis'\n",
    "            elif round_type == 'F':\n",
    "                round_column = 'finals'\n",
    "            \n",
    "            # Marca o time vencedor como \"W\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == winner_id), round_column] = 'W'\n",
    "            \n",
    "            # Marca o time perdedor como \"L\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == loser_id), round_column] = 'L'\n",
    "    \n",
    "    return teams\n",
    "\n",
    "teams_file = corrige_vencedor(df4, df7)\n",
    "\n",
    "players_teams_file = df2.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "\n",
    "players_file = df1[df1['pos'].notna() & (df1['pos'] != '')] # tirar jogadoras com linhas vazias\n",
    "\n",
    "players_file = players_file.drop(columns=['firstseason', 'lastseason', 'deathDate', 'collegeOther']) # dropar firstseason e lastseason porque têm os valores todos iguais, dropar deathDate porque quase ninguém morreu \n",
    "players_file['college'] = players_file['college'].apply(lambda x: 1 if pd.notnull(x) else 0) # substituir college por escolaridade\n",
    "\n",
    "merged_df = pd.merge(players_teams_file, players_file, left_on='playerID', right_on='bioID', how='left') # merge players_teams e players\n",
    "\n",
    "merged_df = merged_df.drop(columns=['bioID']) # tirar bioID porque já temos playerID\n",
    "awards_players_file = df3.drop(columns=['lgID']) # tirar porque é tudo igual\n",
    "teams_file = df4.drop(columns=['lgID', 'divID', 'tmORB','tmDRB','tmTRB','opptmORB','opptmDRB','opptmTRB','seeded']) # dropar lgID e divID porque é igual em tudo e o resto estava tudo a zero\n",
    "teams_file['playoff'] = teams_file['playoff'].apply(lambda x: 1 if x=='Y' else 0) # substituir playoff por valores numericos\n",
    "\n",
    "team_post_file = df5.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "series_post_file = df7.drop(columns=['lgIDWinner', 'lgIDLoser']) # tirar tudo porque é tudo igual\n",
    "coaches_file = df6.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "\n",
    "\n",
    "awards_grouped = awards_players_file.groupby(['playerID', 'year'])['award'].apply(list).reset_index() # agrupamos numa lista todos os awards de cada jogadora em cada ano\n",
    "awards_grouped['award'] = awards_grouped['award'].apply(lambda x: x if isinstance(x, list) else []) # se uma jogadora não tem awards -> lista vazia\n",
    "\n",
    "merged_df = pd.merge(merged_df, awards_grouped, on=['playerID', 'year'], how='left') # merge do dataset que foi merged em cima com o dos awards\n",
    "merged_df['award'] = merged_df['award'].apply(lambda x: x if isinstance(x, list) else []) # se uma jogadora não tem awards -> lista vazia\n",
    "merged_df = pd.merge(merged_df, teams_file, on=['tmID','year'], how = 'left') # merge com o teams\n",
    "\n",
    "merged_df = merged_df.drop(columns=['franchID', 'name']) # dropar franchID porque é igual à sigla da equipa e name porque já temos a sigla\n",
    "\n",
    "merged_df = pd.merge(merged_df, team_post_file, on=['tmID','year'], how = 'left') # merge com o teams post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge do awards com os coaches\n",
    "\n",
    "awards_coaches_file = df3.rename(columns={'playerID': 'coachID'})\n",
    "coach_awards = awards_coaches_file[awards_coaches_file['award'] == 'Coach of the Year']\n",
    "coach_awards_grouped = coach_awards.groupby(['coachID', 'year'])['award'].apply(list).reset_index()\n",
    "coaches_file = pd.merge(coaches_file, coach_awards_grouped, on=['coachID', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['rebounds', 'PostRebounds']) # resultado da célula anterior, os rebounds ofensivos/defensivos mudam de acordo com a posição da jogadora, logo decidimos tirar o total de rebounds\n",
    "merged_df = merged_df.rename(columns={'GP_x': 'GP_player', 'GP_y': 'GP_team'}) # haviam duas colunas com nomes iguais, uma para as jogadoras e outra para as equipas, tinham ficado uma com x e a outra com y então demos rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Novas estatisticas -> PER (Player Effiency Rating)\n",
    "\n",
    "grouped = merged_df.groupby('year').agg({\n",
    "    'o_pts': 'sum',\n",
    "    'o_fga': 'sum',\n",
    "    'o_oreb': 'sum',\n",
    "    'o_to': 'sum',\n",
    "    'o_fta': 'sum',\n",
    "    'o_asts': 'sum',\n",
    "    'o_fgm' : 'sum',\n",
    "    'o_ftm': 'sum',\n",
    "    'o_dreb':'sum',\n",
    "}).reset_index()\n",
    "\n",
    "grouped['VOP'] = grouped['o_pts'] / (grouped['o_fga'] - grouped['o_oreb'] + grouped['o_to'] + 0.44 * grouped['o_fta'])\n",
    "grouped['factor'] = (2 / 3) - (0.5 * (grouped['o_asts'] / grouped['o_fgm'])) / (2 * (grouped['o_fgm'] / grouped['o_ftm']))\n",
    "grouped['DRB%'] = (grouped['o_dreb'] - grouped['o_oreb']) / grouped['o_dreb']\n",
    "\n",
    "uPER_df = merged_df.groupby(['playerID', 'year']).agg({\n",
    "    'minutes': 'sum',     \n",
    "    'threeMade': 'sum',   \n",
    "    'assists': 'sum',     \n",
    "    'fgMade': 'sum',      \n",
    "    'ftMade': 'sum',      \n",
    "    'turnovers': 'sum',   \n",
    "    'fgAttempted': 'sum', \n",
    "    'ftAttempted': 'sum', \n",
    "    'dRebounds': 'sum',   \n",
    "    'oRebounds': 'sum',   \n",
    "    'steals': 'sum',      \n",
    "    'blocks': 'sum',      \n",
    "    'PF': 'sum'           \n",
    "}).reset_index()\n",
    "\n",
    "uPER_df = uPER_df.merge(grouped[['year', 'VOP', 'factor', 'DRB%']], on='year')\n",
    "\n",
    "uPER_df['TRB'] = uPER_df['dRebounds'] + uPER_df['oRebounds']\n",
    "\n",
    "uPER_df['uPER'] = (1 / uPER_df['minutes']) * (\n",
    "    uPER_df['threeMade'] +\n",
    "    (2/3) * uPER_df['assists'] +\n",
    "    (2 - uPER_df['factor'] * (uPER_df['assists'] / uPER_df['fgMade'])) * uPER_df['fgMade'] +\n",
    "    (uPER_df['ftMade'] * 0.5 * (1 + (1 - (uPER_df['assists'] / uPER_df['fgMade'])) + (2/3) * (uPER_df['assists'] / uPER_df['fgMade']))) -\n",
    "    uPER_df['VOP'] * uPER_df['turnovers'] -\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * (uPER_df['fgAttempted'] - uPER_df['fgMade']) -\n",
    "    uPER_df['VOP'] * 0.44 * (0.44 + (0.56 * uPER_df['DRB%'])) * (uPER_df['ftAttempted'] - uPER_df['ftMade']) +\n",
    "    uPER_df['VOP'] * (1 - uPER_df['DRB%']) * uPER_df['TRB'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['oRebounds'] +\n",
    "    uPER_df['VOP'] * uPER_df['steals'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['blocks'] -\n",
    "    uPER_df['PF'] * ((grouped['o_ftm'].mean() / grouped['o_pts'].mean()) - 0.44 * (grouped['o_fta'].mean() / grouped['o_pts'].mean()) * uPER_df['VOP'])\n",
    ")\n",
    "\n",
    "lg_uPER = uPER_df.groupby('year')['uPER'].mean().reset_index()\n",
    "lg_uPER.rename(columns={'uPER': 'lg_uPER'}, inplace=True)\n",
    "\n",
    "uPER_df = uPER_df.merge(lg_uPER, on='year')\n",
    "\n",
    "uPER_df['PER'] = uPER_df['uPER'] * (15 / uPER_df['lg_uPER'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_to_merge = uPER_df[['playerID', 'year', 'PER']]\n",
    "merged_df = merged_df.merge(per_to_merge, on=['playerID', 'year'], how='left') # adicionar o PER ao dataset\n",
    "\n",
    "# novas estatisticas\n",
    "merged_df['TS%'] = (merged_df['points'] / (2 * (merged_df['fgAttempted'] + 0.44 * merged_df['ftAttempted'])))*100\n",
    "merged_df['eFG%'] = ((merged_df['fgMade'] + 0.5 * merged_df['threeMade']) / merged_df['fgAttempted'])*100\n",
    "merged_df['stocks'] = (merged_df['steals'] + merged_df['blocks'])\n",
    "merged_df['dar'] = ((merged_df['steals'] + merged_df['blocks'] + merged_df['oRebounds'] + merged_df['dRebounds'])/merged_df['minutes'])\n",
    "\n",
    "# substituir por 0 colunas vazias\n",
    "merged_df['PER'] = merged_df['PER'].fillna(0)\n",
    "merged_df['TS%'] = merged_df['TS%'].fillna(0)\n",
    "merged_df['eFG%'] = merged_df['eFG%'].fillna(0)\n",
    "merged_df['stocks'] = merged_df['stocks'].fillna(0)\n",
    "merged_df['dar'] = merged_df['dar'].fillna(0)\n",
    "\n",
    "#TODO escalar pelos minutos jogados (per, efg stocks e ts pelo tempo. O dar nao pois já tem em conta os minutos jogados)\n",
    "max_minutes = merged_df['minutes'].max()\n",
    "\n",
    "if max_minutes != 0:\n",
    "    # Multiplicando cada valor de 'TS%' pelo fator (min_minutes / max_minutes)\n",
    "    merged_df['TS%'] = merged_df['TS%'] * (merged_df['minutes'] / max_minutes)\n",
    "    merged_df['eFG%'] = merged_df['eFG%'] * (merged_df['minutes'] / max_minutes)\n",
    "    merged_df['PER'] = merged_df['PER'] * (merged_df['minutes'] / max_minutes)\n",
    "    merged_df['stocks'] = merged_df['stocks'] * (merged_df['minutes'] / max_minutes)\n",
    "\n",
    "#TODO............................................\n",
    "\n",
    "\n",
    "#Equipas que não foram aos playoffs\n",
    "merged_df['W'] = merged_df['W'].fillna(0)\n",
    "merged_df['L'] = merged_df['L'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir valores nominais para valores relativos (one-attribute-per-value conversion)\n",
    "def replaceGameResults(column):\n",
    "    return column.apply(lambda value: '100' if value == 'W' else '010' if value == 'L' else '001')\n",
    "\n",
    "# Aplicar a função para cada coluna específica\n",
    "merged_df['firstRound'] = replaceGameResults(merged_df['firstRound'])\n",
    "merged_df['semis'] = replaceGameResults(merged_df['semis'])\n",
    "merged_df['finals'] = replaceGameResults(merged_df['finals'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queriamos construir o dataset em relação às jogadoras então dropamos imensas colunas relativas à equipa\n",
    "\n",
    "merged_df2 = merged_df.drop(columns=['minutes','threeMade','assists','fgMade','turnovers','fgAttempted','ftAttempted','oRebounds','steals','blocks','PF','o_ftm','o_pts','o_fta','o_pts','o_fga','o_oreb','o_to','o_asts','o_fgm','o_dreb']) # estes atributos já se encontram nas novas colunas criadas\n",
    "merged_df2 = merged_df2.drop(columns=['GP_player','GS','ftMade','threeAttempted','GP_team'])\n",
    "merged_df2 = merged_df2.drop(columns=['o_3pm','o_3pa','o_reb','o_pf','o_stl','o_blk','d_fgm','d_fga','d_ftm','d_fta','d_3pm','d_3pa','d_oreb','d_dreb','d_reb','d_asts','d_pf','d_stl','d_to','d_blk','d_pts'])\n",
    "merged_df2 = merged_df2.drop(columns=['PostGP','PostGS','PostMinutes','PostPoints','PostoRebounds','PostdRebounds','PostAssists','PostSteals','PostBlocks','PostTurnovers','PostPF','PostfgAttempted','PostfgMade','PostftAttempted','PostftMade','PostthreeAttempted','PostthreeMade','PostDQ'])\n",
    "merged_df2 = merged_df2.drop(columns=['arena'])\n",
    "\n",
    "# substituir BirthDate por ano em que nasceram\n",
    "merged_df2['birthDate'] = pd.to_datetime(merged_df['birthDate'], errors='coerce').dt.year\n",
    "merged_df2 = merged_df2.rename(columns={'birthDate': 'birthYear'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df9 = pd.read_csv('datasetsYear11/players_teams.csv')\n",
    "df8 = pd.read_csv('datasetsYear11/coaches.csv')\n",
    "df10 = pd.read_csv('datasetsYear11/teams.csv')\n",
    "\n",
    "# Agrupar coaches por year e team, transformando em listas\n",
    "coaches_grouped = df6.groupby(['year', 'tmID'])['coachID'].apply(list).reset_index()\n",
    "\n",
    "# Fazer o merge com o ficheiro\n",
    "merged_df2 = merged_df2.merge(coaches_grouped, on=['year', 'tmID'], how='left')\n",
    "\n",
    "year11 = df9.merge(df8, on=['tmID','year'], how='left')\n",
    "year11 = year11.merge(df10, on=['tmID','year'], how='left')\n",
    "finalYear11 = year11[['playerID','year','tmID','confID','coachID']]\n",
    "finalYear11 = finalYear11.groupby(['playerID', 'year', 'tmID', 'confID'])['coachID'].apply(list).reset_index()\n",
    "\n",
    "# Adicionar colunas ausentes em finalYear11 com valor 0\n",
    "expected_columns_merged_df2 = merged_df2.columns\n",
    "finalYear11 = finalYear11.reindex(columns=expected_columns_merged_df2, fill_value=0)\n",
    "\n",
    "# Concatenar os datasets\n",
    "merged_df2 = pd.concat([merged_df2, finalYear11], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_stats_prevYear = merged_df2[['playerID','year','PER', 'eFG%', 'TS%','stocks','dRebounds','dar']].drop_duplicates().copy() #TODO Acrescentar aqui mais variaveis\n",
    "\n",
    "players_stats_prevYear['legacy_points'] = merged_df2['won']\n",
    "players_stats_prevYear['year'] = players_stats_prevYear['year'] + 1\n",
    "\n",
    "\n",
    "players_stats_prevYear = players_stats_prevYear.merge(\n",
    "    merged_df2[['playerID', 'year', 'tmID', 'playoff','coachID','confID']], \n",
    "    on=['playerID', 'year'], \n",
    "    how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................Fazer a media por equipa dos valores mas pegando apenas nos 7 melhores jogadores...............\n",
    "\n",
    "# Ordenar os jogadores dentro de cada equipe e ano com base no PER (ou outra métrica)\n",
    "players_stats_prevYear_sorted = players_stats_prevYear.sort_values(by=['tmID', 'year', 'PER'], ascending=[True, True, False])\n",
    "\n",
    "# Agora, calcular a média das métricas apenas para os melhores jogadores\n",
    "team_year_stats = players_stats_prevYear_sorted.groupby(['tmID', 'year', 'playoff','confID']).agg({\n",
    "    'PER': 'mean',\n",
    "    'TS%': 'mean',\n",
    "    'eFG%': 'mean',\n",
    "    'stocks': 'mean',\n",
    "    'dRebounds': 'mean',\n",
    "    'dar': 'mean',\n",
    "    'coachID': 'first',  # Seleciona o primeiro valor da coluna (presumindo que todos sejam iguais para o grupo)\n",
    "    'legacy_points' : 'mean' #Same here\n",
    "}).reset_index()\n",
    "\n",
    "#TODO...........................................................\n",
    "playoff_teams_year11 = [\"WAS\", \"NYL\", \"IND\", \"ATL\", \"SEA\", \"PHO\", \"SAS\", \"LAS\"]\n",
    "\n",
    "team_year_stats.loc[(team_year_stats['year'] == 11) & (team_year_stats['tmID'].isin(playoff_teams_year11)), 'playoff'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['stocks', 'dar', 'eFG%']\n",
      "    tmID  year  playoff confID       eFG%     stocks       dar  \\\n",
      "0    ATL     9      0.0     EA  16.353479  11.605348  0.219663   \n",
      "1    ATL    10      1.0     EA  24.489119  22.914641  0.257224   \n",
      "2    ATL    11      1.0     EA  25.028354  26.471195  0.230543   \n",
      "3    CHA     2      1.0     EA  23.619522  20.759139  0.189456   \n",
      "4    CHA     3      1.0     EA  19.065987  16.319895  0.196333   \n",
      "..   ...   ...      ...    ...        ...        ...       ...   \n",
      "133  WAS     7      1.0     EA  27.033872  24.662075  0.172187   \n",
      "134  WAS     8      0.0     EA  23.979684  24.048747  0.201451   \n",
      "135  WAS     9      0.0     EA  19.347908  18.718676  0.202555   \n",
      "136  WAS    10      1.0     EA  18.927090  15.742800  0.219591   \n",
      "137  WAS    11      1.0     EA  27.300987  30.179092  0.254096   \n",
      "\n",
      "                      coachID  legacy_points  \n",
      "0                [meadoma99w]      18.916667  \n",
      "1                [meadoma99w]       8.444444  \n",
      "2                [meadoma99w]      17.545455  \n",
      "3                [donovan99w]      14.444444  \n",
      "4                [donovan99w]      17.666667  \n",
      "..                        ...            ...  \n",
      "133              [adubari99w]      16.375000  \n",
      "134  [adubari99w, rollitr01w]      15.923077  \n",
      "135  [kenlaje99w, rollitr01w]      14.538462  \n",
      "136              [plankju99w]      12.153846  \n",
      "137              [laceytr99w]      15.000000  \n",
      "\n",
      "[138 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Para depois poder ir buscar as variavies categoricas\n",
    "team_year_stats_copy = team_year_stats.copy()\n",
    "\n",
    "\n",
    "# Selecionar as colunas de interesse\n",
    "features = team_year_stats[['PER', 'TS%', 'eFG%','stocks','dar']]\n",
    "\n",
    "# Normalizar os dados usando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Aplicar o PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Verificar as cargas (coeficientes) dos componentes principais\n",
    "components = pca.components_\n",
    "\n",
    "# Baseado nas cargas, você pode decidir as variáveis mais importantes\n",
    "# Vamos mostrar a importância de cada variável nas componentes principais\n",
    "\n",
    "# Calcular a soma das cargas absolutas para cada variável\n",
    "importance = pd.DataFrame(abs(components), columns=['PER', 'TS%', 'eFG%','stocks','dar'], index=['PC1', 'PC2','PC3']) #TODO mudar aqui as variaveis\n",
    "#TODO meter tantos PC quanto variaves a selecionar\n",
    "importance_sum = importance.sum(axis=0)\n",
    "\n",
    "# Selecionar as duas variáveis mais importantes\n",
    "most_important_features = importance_sum.sort_values(ascending=False).head(3) #TODO mudar aqui o numero de variaveis\n",
    "\n",
    "# Exibir apenas os nomes das variáveis mais importantes\n",
    "important_variable_names = most_important_features.index.tolist()\n",
    "\n",
    "# Inicializar a lista de componentes a serem removidos\n",
    "components_to_drop = ['PER', 'TS%', 'eFG%','stocks','dRebounds','dar'] #TODO mudar aqui as variavies (TODOS OS ATRIBUTOS AQUI)\n",
    "#TODO nao esquecer de aqui adicionar as nao numericas, como os coaches\n",
    "\n",
    "# Remover as variáveis mais importantes da lista de componentes a serem removidos\n",
    "components_to_drop = [col for col in components_to_drop if col not in important_variable_names]\n",
    "\n",
    "team_year_stats=team_year_stats.drop(columns=components_to_drop)\n",
    "\n",
    "#Adicionar as variaveis categoricas, como os coaches, que nao passaram no processo de PCA\n",
    "team_year_stats['coachID'] = team_year_stats_copy['coachID']\n",
    "#team_year_stats['legacy_points'] = team_year_stats_copy['legacy_points']\n",
    "#important_variable_names.append('legacy_points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  tmID  year  playoff confID      eFG%    stocks       dar       coachID  \\\n",
      "0  ATL     9      0.0     EA  0.130134  0.090562  0.625691  [meadoma99w]   \n",
      "1  ATL    10      1.0     EA  0.478069  0.395795  0.911758  [meadoma99w]   \n",
      "2  ATL    11      1.0     EA  0.501130  0.491785  0.708553  [meadoma99w]   \n",
      "3  CHA     2      1.0     EA  0.440879  0.337619  0.395635  [donovan99w]   \n",
      "4  CHA     3      1.0     EA  0.246139  0.217806  0.448008  [donovan99w]   \n",
      "\n",
      "   legacy_points  \n",
      "0       0.593575  \n",
      "1       0.067039  \n",
      "2       0.524632  \n",
      "3       0.368715  \n",
      "4       0.530726  \n"
     ]
    }
   ],
   "source": [
    "# Normalizar os dados \n",
    "\n",
    "# Selecionar apenas colunas numéricas\n",
    "# Lista de colunas a normalizar\n",
    "columns_to_normalize = ['eFG%', 'stocks','dar','legacy_points'] #TODO baseado nas colunas selecionadas do dataset de cima\n",
    "\n",
    "# Aplicar a normalização apenas nas colunas selecionadas\n",
    "team_year_stats[columns_to_normalize] = team_year_stats[columns_to_normalize].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir o dataset para treino, validacao e teste \n",
    "\n",
    "dataset_treino = team_year_stats[(team_year_stats['year'] >= 0) & (team_year_stats['year'] <= 10)]\n",
    "dataset_teste = team_year_stats[team_year_stats['year'] == 11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Features (X) e alvo (y)\n",
    "X_treino = dataset_treino[important_variable_names] #Antigamente 'PER', 'TS%', 'eFG%'\n",
    "y_treino = dataset_treino['playoff']\n",
    "\n",
    "X_teste = dataset_teste[important_variable_names]\n",
    "y_teste = dataset_teste['playoff']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar ficheiros para guardar resultados e avaliar\n",
    "dataset_resultados = pd.DataFrame({})\n",
    "\n",
    "dataset_resultados[\"playoff\"] = dataset_teste[\"playoff\"]\n",
    "\n",
    "dataset_resultados[\"tmID\"] = dataset_teste[\"tmID\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\maxbp\\AppData\\Local\\Temp\\ipykernel_25624\\4200620747.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão com Extra Trees\n",
    "modelo = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"ExtraTreesRegressor\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Seleciona os valores para cada equipa da conf\n",
    "dataset_para_obter_confID = team_year_stats\n",
    "y_unique = dataset_para_obter_confID[['tmID', 'confID']].drop_duplicates()\n",
    "\n",
    "# Adiciona a confID ao dataset x com base na correspondência com y\n",
    "dataset_resultados = dataset_resultados.merge(y_unique[['tmID', 'confID']], on='tmID', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame(dataset_resultados)\n",
    "\n",
    "# Inicializar lista para armazenar os resultados\n",
    "resultados = []\n",
    "\n",
    "# Número de melhores previsões para considerar como 1 por conferência\n",
    "top_n_conferencia = 4\n",
    "\n",
    "# Loop sobre cada coluna do modelo (excluindo a coluna 'playoff')\n",
    "for modelo in df.columns[2:-1]:\n",
    "    # Cálculo de métricas de regressão\n",
    "    mse = mean_squared_error(df[\"playoff\"], df[modelo])\n",
    "    mae = mean_absolute_error(df[\"playoff\"], df[modelo])\n",
    "    r2 = abs(r2_score(df[\"playoff\"], df[modelo]))\n",
    "    \n",
    "    # Binarização dos valores (top 4 melhores de cada conferência como 1)\n",
    "    y_true = (df[\"playoff\"] >= 0.5).astype(int)\n",
    "    y_pred = pd.Series(0, index=df.index)  # Inicializa com 0 para todos\n",
    "    \n",
    "    # Separa os dados por conferência\n",
    "    conferencia_1 = df[df[\"confID\"] == \"WE\"]\n",
    "    conferencia_2 = df[df[\"confID\"] == \"EA\"]\n",
    "    \n",
    "    # Obtém os índices dos top 4 de cada conferência\n",
    "    top_indices_1 = conferencia_1[modelo].nlargest(top_n_conferencia).index\n",
    "    top_indices_2 = conferencia_2[modelo].nlargest(top_n_conferencia).index\n",
    "    \n",
    "    # Define 1 para os top 4 de cada conferência\n",
    "    y_pred.loc[top_indices_1] = 1\n",
    "    y_pred.loc[top_indices_2] = 1\n",
    "    \n",
    "    # Cálculo de métricas de classificação\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Adicionar os resultados à lista\n",
    "    resultados.append({\n",
    "        \"Modelo\": modelo,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "resultados_4_melhores = pd.DataFrame(resultados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error total: 3.6099773242630393\n",
      "Adj total: 7.999999999999999\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>playoff</th>\n",
       "      <th>tmID</th>\n",
       "      <th>adj_ExtraTreesRegressor</th>\n",
       "      <th>error_ExtraTreesRegressor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>0.770975</td>\n",
       "      <td>0.229025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>CHI</td>\n",
       "      <td>0.544218</td>\n",
       "      <td>0.544218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>CON</td>\n",
       "      <td>0.326531</td>\n",
       "      <td>0.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>IND</td>\n",
       "      <td>0.907029</td>\n",
       "      <td>0.092971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>LAS</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>MIN</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.897959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td>NYL</td>\n",
       "      <td>0.408163</td>\n",
       "      <td>0.591837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>PHO</td>\n",
       "      <td>0.625850</td>\n",
       "      <td>0.374150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SAS</td>\n",
       "      <td>0.907029</td>\n",
       "      <td>0.092971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>SEA</td>\n",
       "      <td>0.879819</td>\n",
       "      <td>0.120181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.0</td>\n",
       "      <td>TUL</td>\n",
       "      <td>0.036281</td>\n",
       "      <td>0.036281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>0.807256</td>\n",
       "      <td>0.192744</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    playoff tmID  adj_ExtraTreesRegressor  error_ExtraTreesRegressor\n",
       "0       1.0  ATL                 0.770975                   0.229025\n",
       "1       0.0  CHI                 0.544218                   0.544218\n",
       "2       0.0  CON                 0.326531                   0.326531\n",
       "3       1.0  IND                 0.907029                   0.092971\n",
       "4       1.0  LAS                 0.888889                   0.111111\n",
       "5       0.0  MIN                 0.897959                   0.897959\n",
       "6       1.0  NYL                 0.408163                   0.591837\n",
       "7       1.0  PHO                 0.625850                   0.374150\n",
       "8       1.0  SAS                 0.907029                   0.092971\n",
       "9       1.0  SEA                 0.879819                   0.120181\n",
       "10      0.0  TUL                 0.036281                   0.036281\n",
       "11      1.0  WAS                 0.807256                   0.192744"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identificar as colunas dos modelos (exceto playoff, tmID e confID)\n",
    "model_columns = [\n",
    "    col for col in dataset_resultados.columns \n",
    "    if col not in ['playoff', 'tmID', 'confID'] and not col.startswith(('adj_', 'error_'))\n",
    "]\n",
    "\n",
    "# Aplicar a transformação para cada coluna de modelo\n",
    "for col in model_columns:\n",
    "    # Calcular o valor ajustado (adj. pred)\n",
    "    dataset_resultados[f'adj_{col}'] = 8 * dataset_resultados[col] / dataset_resultados[col].sum()\n",
    "    \n",
    "    # Calcular o erro absoluto em relação a 'label'\n",
    "    dataset_resultados[f'error_{col}'] = abs(dataset_resultados[f'adj_{col}'] - dataset_resultados['playoff'])\n",
    "\n",
    "# Identificar as colunas de erro (que começam com 'error_')\n",
    "error_columns = [col for col in dataset_resultados.columns if col.startswith('error_')]\n",
    "\n",
    "# Criar um dicionário com os erros finais para cada modelo\n",
    "final_errors = {\n",
    "    col.replace('error_', ''): dataset_resultados[col].sum() for col in error_columns\n",
    "}\n",
    "\n",
    "# Criar um DataFrame com os resultados finais\n",
    "error_summary = pd.DataFrame(final_errors.items(), columns=['Model', 'Final Error'])\n",
    "\n",
    "error_summary.head(15)\n",
    "\n",
    "teste = dataset_resultados[['playoff','tmID','adj_ExtraTreesRegressor','error_ExtraTreesRegressor']]\n",
    "\n",
    "print(\"Error total:\", teste['error_ExtraTreesRegressor'].sum())\n",
    "print(\"Adj total:\", teste['adj_ExtraTreesRegressor'].sum())\n",
    "teste.head(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
