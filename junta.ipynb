{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df1 = pd.read_csv('datasets/players.csv')\n",
    "df2 = pd.read_csv('datasets/players_teams.csv')\n",
    "df3 = pd.read_csv('datasets/awards_players.csv')\n",
    "df4 = pd.read_csv('datasets/teams.csv')\n",
    "df5 = pd.read_csv('datasets/teams_post.csv')\n",
    "df6 = pd.read_csv('datasets/coaches.csv')\n",
    "df7 = pd.read_csv('datasets/series_post.csv')\n",
    "\n",
    "def corrige_vencedor(teams, series_post):\n",
    "    # Itera sobre cada rodada ('F', 'CF', 'FR') para ajustar cada fase dos playoffs\n",
    "    for round_type in ['FR', 'CF', 'F']:\n",
    "        # Filtra a série específica da rodada\n",
    "        series_round = series_post[series_post['round'] == round_type]\n",
    "        \n",
    "        # Atualiza cada série individualmente\n",
    "        for _, row in series_round.iterrows():\n",
    "            year = row['year']\n",
    "            winner_id = row['tmIDWinner']\n",
    "            loser_id = row['tmIDLoser']\n",
    "            \n",
    "            # Define as colunas que correspondem às rodadas\n",
    "            if round_type == 'FR':\n",
    "                round_column = 'firstRound'\n",
    "            elif round_type == 'CF':\n",
    "                round_column = 'semis'\n",
    "            elif round_type == 'F':\n",
    "                round_column = 'finals'\n",
    "            \n",
    "            # Marca o time vencedor como \"W\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == winner_id), round_column] = 'W'\n",
    "            \n",
    "            # Marca o time perdedor como \"L\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == loser_id), round_column] = 'L'\n",
    "    \n",
    "    return teams\n",
    "\n",
    "teams_file = corrige_vencedor(df4, df7)\n",
    "\n",
    "players_teams_file = df2.drop(columns=['lgID'])\n",
    "players_file = df1[df1['pos'].notna() & (df1['pos'] != '')]\n",
    "players_file = players_file.drop(columns=['firstseason', 'lastseason', 'deathDate', 'collegeOther'])\n",
    "players_file['college'] = players_file['college'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "merged_df = pd.merge(players_teams_file, players_file, left_on='playerID', right_on='bioID', how='left')\n",
    "merged_df = merged_df.drop(columns=['bioID'])\n",
    "awards_players_file = df3.drop(columns=['lgID'])\n",
    "teams_file = df4.drop(columns=['lgID', 'divID', 'tmORB','tmDRB','tmTRB','opptmORB','opptmDRB','opptmTRB','seeded'])\n",
    "teams_file['playoff'] = teams_file['playoff'].apply(lambda x: 1 if x=='Y' else 0)\n",
    "\n",
    "team_post_file = df5.drop(columns=['lgID'])\n",
    "series_post_file = df7.drop(columns=['lgIDWinner', 'lgIDLoser'])\n",
    "coaches_file = df6.drop(columns=['lgID'])\n",
    "\n",
    "\n",
    "awards_grouped = awards_players_file.groupby(['playerID', 'year'])['award'].apply(list).reset_index()\n",
    "awards_grouped['award'] = awards_grouped['award'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "merged_df = pd.merge(merged_df, awards_grouped, on=['playerID', 'year'], how='left')\n",
    "merged_df['award'] = merged_df['award'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "merged_df = pd.merge(merged_df, teams_file, on=['tmID','year'], how = 'left')\n",
    "\n",
    "merged_df = merged_df.drop(columns=['franchID', 'name'])\n",
    "\n",
    "merged_df = pd.merge(merged_df, team_post_file, on=['tmID','year'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_count_per_team_year = merged_df.groupby(['year', 'tmID'])['playerID'].nunique().reset_index()\n",
    "player_count_per_team_year.columns = ['Year', 'Team', 'PlayerCount']\n",
    "\n",
    "# print(player_count_per_team_year)\n",
    "\n",
    "# Calculate min, max, and average player count for each year\n",
    "summary_stats = player_count_per_team_year.groupby('Year')['PlayerCount'].agg(['min', 'max', 'mean']).reset_index()\n",
    "summary_stats.columns = ['Year', 'MinPlayerCount', 'MaxPlayerCount', 'AvgPlayerCount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_coaches_file = df3.rename(columns={'playerID': 'coachID'})\n",
    "coach_awards = awards_coaches_file[awards_coaches_file['award'] == 'Coach of the Year']\n",
    "coach_awards_grouped = coach_awards.groupby(['coachID', 'year'])['award'].apply(list).reset_index()\n",
    "coaches_file = pd.merge(coaches_file, coach_awards_grouped, on=['coachID', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_oRebounds_by_pos = merged_df.groupby('pos')['oRebounds'].mean().reset_index()\n",
    "avg_dRebounds_by_pos = merged_df.groupby('pos')['dRebounds'].mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['rebounds', 'PostRebounds'])\n",
    "merged_df = merged_df.rename(columns={'GP_x': 'GP_player', 'GP_y': 'GP_team'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = merged_df.groupby('year').agg({\n",
    "    'o_pts': 'sum',\n",
    "    'o_fga': 'sum',\n",
    "    'o_oreb': 'sum',\n",
    "    'o_to': 'sum',\n",
    "    'o_fta': 'sum',\n",
    "    'o_asts': 'sum',\n",
    "    'o_fgm' : 'sum',\n",
    "    'o_ftm': 'sum',\n",
    "    'o_dreb':'sum',\n",
    "}).reset_index()\n",
    "\n",
    "grouped['VOP'] = grouped['o_pts'] / (grouped['o_fga'] - grouped['o_oreb'] + grouped['o_to'] + 0.44 * grouped['o_fta'])\n",
    "grouped['factor'] = (2 / 3) - (0.5 * (grouped['o_asts'] / grouped['o_fgm'])) / (2 * (grouped['o_fgm'] / grouped['o_ftm']))\n",
    "grouped['DRB%'] = (grouped['o_dreb'] - grouped['o_oreb']) / grouped['o_dreb']\n",
    "\n",
    "uPER_df = merged_df.groupby(['playerID', 'year']).agg({\n",
    "    'minutes': 'sum',     \n",
    "    'threeMade': 'sum',   \n",
    "    'assists': 'sum',     \n",
    "    'fgMade': 'sum',      \n",
    "    'ftMade': 'sum',      \n",
    "    'turnovers': 'sum',   \n",
    "    'fgAttempted': 'sum', \n",
    "    'ftAttempted': 'sum', \n",
    "    'dRebounds': 'sum',   \n",
    "    'oRebounds': 'sum',   \n",
    "    'steals': 'sum',      \n",
    "    'blocks': 'sum',      \n",
    "    'PF': 'sum'           \n",
    "}).reset_index()\n",
    "\n",
    "uPER_df = uPER_df.merge(grouped[['year', 'VOP', 'factor', 'DRB%']], on='year')\n",
    "\n",
    "uPER_df['TRB'] = uPER_df['dRebounds'] + uPER_df['oRebounds']\n",
    "\n",
    "uPER_df['uPER'] = (1 / uPER_df['minutes']) * (\n",
    "    uPER_df['threeMade'] +\n",
    "    (2/3) * uPER_df['assists'] +\n",
    "    (2 - uPER_df['factor'] * (uPER_df['assists'] / uPER_df['fgMade'])) * uPER_df['fgMade'] +\n",
    "    (uPER_df['ftMade'] * 0.5 * (1 + (1 - (uPER_df['assists'] / uPER_df['fgMade'])) + (2/3) * (uPER_df['assists'] / uPER_df['fgMade']))) -\n",
    "    uPER_df['VOP'] * uPER_df['turnovers'] -\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * (uPER_df['fgAttempted'] - uPER_df['fgMade']) -\n",
    "    uPER_df['VOP'] * 0.44 * (0.44 + (0.56 * uPER_df['DRB%'])) * (uPER_df['ftAttempted'] - uPER_df['ftMade']) +\n",
    "    uPER_df['VOP'] * (1 - uPER_df['DRB%']) * uPER_df['TRB'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['oRebounds'] +\n",
    "    uPER_df['VOP'] * uPER_df['steals'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['blocks'] -\n",
    "    uPER_df['PF'] * ((grouped['o_ftm'].mean() / grouped['o_pts'].mean()) - 0.44 * (grouped['o_fta'].mean() / grouped['o_pts'].mean()) * uPER_df['VOP'])\n",
    ")\n",
    "\n",
    "lg_uPER = uPER_df.groupby('year')['uPER'].mean().reset_index()\n",
    "lg_uPER.rename(columns={'uPER': 'lg_uPER'}, inplace=True)\n",
    "\n",
    "uPER_df = uPER_df.merge(lg_uPER, on='year')\n",
    "\n",
    "uPER_df['PER'] = uPER_df['uPER'] * (15 / uPER_df['lg_uPER'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_to_merge = uPER_df[['playerID', 'year', 'PER']]\n",
    "merged_df = merged_df.merge(per_to_merge, on=['playerID', 'year'], how='left')\n",
    "\n",
    "merged_df['TS%'] = (merged_df['points'] / (2 * (merged_df['fgAttempted'] + 0.44 * merged_df['ftAttempted'])))*100\n",
    "merged_df['eFG%'] = ((merged_df['fgMade'] + 0.5 * merged_df['threeMade']) / merged_df['fgAttempted'])*100\n",
    "merged_df['stocks'] = (merged_df['steals'] + merged_df['blocks'])\n",
    "\n",
    "merged_df['PER'] = merged_df['PER'].fillna(0)\n",
    "merged_df['TS%'] = merged_df['TS%'].fillna(0)\n",
    "merged_df['eFG%'] = merged_df['eFG%'].fillna(0)\n",
    "merged_df['stocks'] = merged_df['stocks'].fillna(0)\n",
    "\n",
    "#Equipas que não foram aos playoffs\n",
    "merged_df['W'] = merged_df['W'].fillna(0)\n",
    "merged_df['L'] = merged_df['L'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_for_each_column(dataset):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    if numeric_columns.empty:\n",
    "        print(\"No numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        numeric_columns.boxplot(figsize=(10, 6))\n",
    "        plt.title(\"Boxplot for all numeric columns\")\n",
    "        plt.xticks(rotation=45)  # Rotation in x, if necessary\n",
    "        plt.show()\n",
    "\n",
    "def pearson_correlation(dataset, size_x, size_y):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    \n",
    "    if numeric_columns.empty:\n",
    "        print(\"Nenhuma coluna numérica encontrada no dataset.\")\n",
    "    else:\n",
    "        # Correlation matrix\n",
    "        correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "        # View\n",
    "        plt.figure(figsize=(size_x, size_y))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Pearson-correlation')\n",
    "        plt.show()\n",
    "\n",
    "def bar_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            value_counts = non_numeric_columns[column].value_counts()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f\"Bar chart for '{column}'\")\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "            plt.show()\n",
    "\n",
    "# Pie-chart for each column\n",
    "def pie_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    \n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            # Count elements from different categories\n",
    "            category_counts = dataset[column].value_counts()\n",
    "            \n",
    "            # Pie-chart\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            category_counts.plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "            plt.title(f'Distribution of {column}')\n",
    "            plt.ylabel('')  # Remove o rótulo do eixo Y\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#box_plot_for_each_column(merged_df)\n",
    "#box_plot_for_each_column(coaches_file)\n",
    "#box_plot_for_each_column(series_post_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearson_correlation(merged_df, 100, 80)\n",
    "#pearson_correlation(coaches_file, 8, 6)\n",
    "#pearson_correlation(series_post_file, 8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar_chart_for_each_column(merged_df)\n",
    "#bar_chart_for_each_column(coaches_file)\n",
    "#bar_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie_chart_for_each_column(merged_df)\n",
    "#pie_chart_for_each_column(coaches_file)\n",
    "#pie_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceGameResults(column):\n",
    "    return column.apply(lambda value: '100' if value == 'W' else '010' if value == 'L' else '001')\n",
    "\n",
    "# Aplicar a função para cada coluna específica\n",
    "merged_df['firstRound'] = replaceGameResults(merged_df['firstRound'])\n",
    "merged_df['semis'] = replaceGameResults(merged_df['semis'])\n",
    "merged_df['finals'] = replaceGameResults(merged_df['finals'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df.to_csv('cleanDatasets/players_and_teams.csv', index=False)\n",
    "coaches_file.to_csv('cleanDatasets/coaches_and_awards.csv', index=False)\n",
    "series_post_file.to_csv('cleanDatasets/series_post.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = merged_df.drop(columns=['minutes','points','threeMade','assists','fgMade','turnovers','fgAttempted','ftAttempted','oRebounds','steals','blocks','PF','o_ftm','o_pts','o_fta','o_pts','o_fga','o_oreb','o_to','o_asts','o_fgm','o_dreb'])\n",
    "merged_df2 = merged_df2.drop(columns=['GP_player','GS','ftMade','threeAttempted','GP_team'])\n",
    "merged_df2 = merged_df2.drop(columns=['o_3pm','o_3pa','o_reb','o_pf','o_stl','o_blk','d_fgm','d_fga','d_ftm','d_fta','d_3pm','d_3pa','d_oreb','d_dreb','d_reb','d_asts','d_pf','d_stl','d_to','d_blk','d_pts'])\n",
    "merged_df2 = merged_df2.drop(columns=['PostGP','PostGS','PostMinutes','PostPoints','PostoRebounds','PostdRebounds','PostAssists','PostSteals','PostBlocks','PostTurnovers','PostPF','PostfgAttempted','PostfgMade','PostftAttempted','PostftMade','PostthreeAttempted','PostthreeMade','PostDQ'])\n",
    "merged_df2 = merged_df2.drop(columns=['arena'])\n",
    "\n",
    "merged_df2['birthDate'] = pd.to_datetime(merged_df['birthDate'], errors='coerce').dt.year\n",
    "merged_df2 = merged_df2.rename(columns={'birthDate': 'birthYear'})\n",
    "\n",
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df2.to_csv('cleanDatasets/advancedstatistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_stats_prevYear = merged_df2[['playerID','year','PER', 'eFG%', 'TS%','stocks','dRebounds']].drop_duplicates().copy()\n",
    "players_stats_prevYear['year'] = players_stats_prevYear['year'] + 1\n",
    "\n",
    "\n",
    "players_stats_prevYear = players_stats_prevYear.merge(\n",
    "    merged_df2[['playerID', 'year', 'tmID', 'playoff']], \n",
    "    on=['playerID', 'year'], \n",
    "    how='left')\n",
    "\n",
    "players_stats_prevYear.to_csv('cleanDatasets/players_stats_prevYear.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a média de PER, TS%, e eFG% por equipe e ano\n",
    "team_year_stats = players_stats_prevYear.groupby(['tmID', 'year','playoff'])[['PER', 'TS%', 'eFG%']].mean().reset_index() # Tirei os stocks e o dRebounds\n",
    "\n",
    "# Salva o novo dataset em um arquivo CSV\n",
    "team_year_stats.to_csv('cleanDatasets/team_year_stats.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Selecionar as colunas para normalizar\n",
    "cols_to_normalize = ['PER', 'TS%', 'eFG%'] # Tirei os stocks e o dRebounds\n",
    "\n",
    "# Inicializar o MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Aplicar a normalização apenas nas colunas especificadas\n",
    "team_year_stats[cols_to_normalize] = scaler.fit_transform(team_year_stats[cols_to_normalize])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir o dataset para treino, validacao e teste \n",
    "\n",
    "dataset_treino = team_year_stats[(team_year_stats['year'] >= 7) & (team_year_stats['year'] <= 9)]\n",
    "dataset_teste = team_year_stats[team_year_stats['year'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Features (X) e alvo (y)\n",
    "X_treino = dataset_treino[['PER', 'TS%', 'eFG%']]\n",
    "y_treino = dataset_treino['playoff']\n",
    "\n",
    "X_teste = dataset_teste[['PER', 'TS%', 'eFG%']]\n",
    "y_teste = dataset_teste['playoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) na validação: 0.2102923076923076\n",
      "R² score na validação: 0.11151500000000047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8349/3804687974.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmID</th>\n",
       "      <th>year</th>\n",
       "      <th>playoff</th>\n",
       "      <th>PER</th>\n",
       "      <th>TS%</th>\n",
       "      <th>eFG%</th>\n",
       "      <th>playoff_previsto</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATL</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.725284</td>\n",
       "      <td>0.585533</td>\n",
       "      <td>0.673958</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CHI</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400577</td>\n",
       "      <td>0.501190</td>\n",
       "      <td>0.570813</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>CON</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.797845</td>\n",
       "      <td>0.749171</td>\n",
       "      <td>0.789321</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>DET</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.720021</td>\n",
       "      <td>0.649269</td>\n",
       "      <td>0.576253</td>\n",
       "      <td>0.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>IND</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401606</td>\n",
       "      <td>0.277840</td>\n",
       "      <td>0.295985</td>\n",
       "      <td>0.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>LAS</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.896107</td>\n",
       "      <td>0.616340</td>\n",
       "      <td>0.656045</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>MIN</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.753603</td>\n",
       "      <td>0.766261</td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>NYL</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.655678</td>\n",
       "      <td>0.717547</td>\n",
       "      <td>0.776724</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>PHO</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.713154</td>\n",
       "      <td>0.592304</td>\n",
       "      <td>0.642979</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>SAC</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.559764</td>\n",
       "      <td>0.738772</td>\n",
       "      <td>0.684971</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>SAS</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.615207</td>\n",
       "      <td>0.762366</td>\n",
       "      <td>0.793953</td>\n",
       "      <td>0.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>SEA</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.354439</td>\n",
       "      <td>0.429093</td>\n",
       "      <td>0.499190</td>\n",
       "      <td>0.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>WAS</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.401577</td>\n",
       "      <td>0.418095</td>\n",
       "      <td>0.515372</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    tmID  year  playoff       PER       TS%      eFG%  playoff_previsto\n",
       "1    ATL    10      1.0  0.725284  0.585533  0.673958              0.90\n",
       "11   CHI    10      0.0  0.400577  0.501190  0.570813              0.41\n",
       "21   CON    10      0.0  0.797845  0.749171  0.789321              0.75\n",
       "30   DET    10      1.0  0.720021  0.649269  0.576253              0.64\n",
       "47   IND    10      1.0  0.401606  0.277840  0.295985              0.59\n",
       "56   LAS    10      1.0  0.896107  0.616340  0.656045              0.72\n",
       "67   MIN    10      0.0  1.000000  0.753603  0.766261              0.78\n",
       "76   NYL    10      0.0  0.655678  0.717547  0.776724              0.58\n",
       "87   PHO    10      1.0  0.713154  0.592304  0.642979              0.89\n",
       "98   SAC    10      0.0  0.559764  0.738772  0.684971              0.62\n",
       "105  SAS    10      1.0  0.615207  0.762366  0.793953              0.85\n",
       "114  SEA    10      1.0  0.354439  0.429093  0.499190              0.62\n",
       "125  WAS    10      1.0  0.401577  0.418095  0.515372              0.67"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão\n",
    "modelo = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Treinar com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Avaliar o modelo com métricas de regressão\n",
    "mse = mean_squared_error(y_teste, y_pred_teste)\n",
    "r2 = r2_score(y_teste, y_pred_teste)\n",
    "\n",
    "print(\"Mean Squared Error (MSE) na validação:\", mse)\n",
    "print(\"R² score na validação:\", r2)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "# Exibir as primeiras linhas para verificar\n",
    "dataset_teste.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) no teste: 0.31103617016086105\n",
      "R² score no teste: -0.3141278189296377\n",
      "    tmID  year  playoff       PER       TS%      eFG%  playoff_previsto\n",
      "1    ATL    10      1.0  0.725284  0.585533  0.673958          0.956449\n",
      "11   CHI    10      0.0  0.400577  0.501190  0.570813          0.878005\n",
      "21   CON    10      0.0  0.797845  0.749171  0.789321          0.873907\n",
      "30   DET    10      1.0  0.720021  0.649269  0.576253          0.680881\n",
      "47   IND    10      1.0  0.401606  0.277840  0.295985          0.740968\n",
      "56   LAS    10      1.0  0.896107  0.616340  0.656045          0.902947\n",
      "67   MIN    10      0.0  1.000000  0.753603  0.766261          0.852928\n",
      "76   NYL    10      0.0  0.655678  0.717547  0.776724          0.810677\n",
      "87   PHO    10      1.0  0.713154  0.592304  0.642979          1.017318\n",
      "98   SAC    10      0.0  0.559764  0.738772  0.684971          0.957079\n",
      "105  SAS    10      1.0  0.615207  0.762366  0.793953          0.931131\n",
      "114  SEA    10      1.0  0.354439  0.429093  0.499190          0.891044\n",
      "125  WAS    10      1.0  0.401577  0.418095  0.515372          0.895045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8349/947436473.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_teste['playoff_previsto'] = y_pred_teste\n"
     ]
    }
   ],
   "source": [
    "# Inicializar o modelo de regressão com Gradient Boosting\n",
    "modelo = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Avaliar o modelo com métricas de regressão\n",
    "mse = mean_squared_error(y_teste, y_pred_teste)\n",
    "r2 = r2_score(y_teste, y_pred_teste)\n",
    "\n",
    "# Exibir as métricas\n",
    "print(\"Mean Squared Error (MSE) no teste:\", mse)\n",
    "print(\"R² score no teste:\", r2)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "# Exibir as primeiras linhas para verificar\n",
    "print(dataset_teste.head(15))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
