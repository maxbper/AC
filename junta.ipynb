{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df1 = pd.read_csv('datasets/players.csv')\n",
    "df2 = pd.read_csv('datasets/players_teams.csv')\n",
    "df3 = pd.read_csv('datasets/awards_players.csv')\n",
    "df4 = pd.read_csv('datasets/teams.csv')\n",
    "df5 = pd.read_csv('datasets/teams_post.csv')\n",
    "df6 = pd.read_csv('datasets/coaches.csv')\n",
    "df7 = pd.read_csv('datasets/series_post.csv')\n",
    "\n",
    "def corrige_vencedor(teams, series_post):\n",
    "    # Itera sobre cada rodada ('F', 'CF', 'FR') para ajustar cada fase dos playoffs\n",
    "    for round_type in ['FR', 'CF', 'F']:\n",
    "        # Filtra a série específica da rodada\n",
    "        series_round = series_post[series_post['round'] == round_type]\n",
    "        \n",
    "        # Atualiza cada série individualmente\n",
    "        for _, row in series_round.iterrows():\n",
    "            year = row['year']\n",
    "            winner_id = row['tmIDWinner']\n",
    "            loser_id = row['tmIDLoser']\n",
    "            \n",
    "            # Define as colunas que correspondem às rodadas\n",
    "            if round_type == 'FR':\n",
    "                round_column = 'firstRound'\n",
    "            elif round_type == 'CF':\n",
    "                round_column = 'semis'\n",
    "            elif round_type == 'F':\n",
    "                round_column = 'finals'\n",
    "            \n",
    "            # Marca o time vencedor como \"W\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == winner_id), round_column] = 'W'\n",
    "            \n",
    "            # Marca o time perdedor como \"L\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == loser_id), round_column] = 'L'\n",
    "    \n",
    "    return teams\n",
    "\n",
    "teams_file = corrige_vencedor(df4, df7)\n",
    "\n",
    "players_teams_file = df2.drop(columns=['lgID'])\n",
    "players_file = df1[df1['pos'].notna() & (df1['pos'] != '')]\n",
    "players_file = players_file.drop(columns=['firstseason', 'lastseason', 'deathDate', 'collegeOther'])\n",
    "players_file['college'] = players_file['college'].apply(lambda x: 1 if pd.notnull(x) else 0)\n",
    "merged_df = pd.merge(players_teams_file, players_file, left_on='playerID', right_on='bioID', how='left')\n",
    "merged_df = merged_df.drop(columns=['bioID'])\n",
    "awards_players_file = df3.drop(columns=['lgID'])\n",
    "teams_file = df4.drop(columns=['lgID', 'divID', 'tmORB','tmDRB','tmTRB','opptmORB','opptmDRB','opptmTRB','seeded'])\n",
    "teams_file['playoff'] = teams_file['playoff'].apply(lambda x: 1 if x=='Y' else 0)\n",
    "\n",
    "team_post_file = df5.drop(columns=['lgID'])\n",
    "series_post_file = df7.drop(columns=['lgIDWinner', 'lgIDLoser'])\n",
    "coaches_file = df6.drop(columns=['lgID'])\n",
    "\n",
    "\n",
    "awards_grouped = awards_players_file.groupby(['playerID', 'year'])['award'].apply(list).reset_index()\n",
    "awards_grouped['award'] = awards_grouped['award'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "\n",
    "merged_df = pd.merge(merged_df, awards_grouped, on=['playerID', 'year'], how='left')\n",
    "merged_df['award'] = merged_df['award'].apply(lambda x: x if isinstance(x, list) else [])\n",
    "merged_df = pd.merge(merged_df, teams_file, on=['tmID','year'], how = 'left')\n",
    "\n",
    "merged_df = merged_df.drop(columns=['franchID', 'name'])\n",
    "\n",
    "merged_df = pd.merge(merged_df, team_post_file, on=['tmID','year'], how = 'left')\n",
    "\n",
    "print(merged_df)\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_count_per_team_year = merged_df.groupby(['year', 'tmID'])['playerID'].nunique().reset_index()\n",
    "player_count_per_team_year.columns = ['Year', 'Team', 'PlayerCount']\n",
    "\n",
    "# print(player_count_per_team_year)\n",
    "\n",
    "# Calculate min, max, and average player count for each year\n",
    "summary_stats = player_count_per_team_year.groupby('Year')['PlayerCount'].agg(['min', 'max', 'mean']).reset_index()\n",
    "summary_stats.columns = ['Year', 'MinPlayerCount', 'MaxPlayerCount', 'AvgPlayerCount']\n",
    "\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "awards_coaches_file = df3.rename(columns={'playerID': 'coachID'})\n",
    "coach_awards = awards_coaches_file[awards_coaches_file['award'] == 'Coach of the Year']\n",
    "coach_awards_grouped = coach_awards.groupby(['coachID', 'year'])['award'].apply(list).reset_index()\n",
    "coaches_file = pd.merge(coaches_file, coach_awards_grouped, on=['coachID', 'year'], how='left')\n",
    "\n",
    "print(coaches_file)\n",
    "coaches_file.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_oRebounds_by_pos = merged_df.groupby('pos')['oRebounds'].mean().reset_index()\n",
    "avg_dRebounds_by_pos = merged_df.groupby('pos')['dRebounds'].mean().reset_index()\n",
    "\n",
    "print('--------------------')\n",
    "print(avg_oRebounds_by_pos)\n",
    "print('--------------------')\n",
    "print(avg_dRebounds_by_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['rebounds', 'PostRebounds'])\n",
    "merged_df = merged_df.rename(columns={'GP_x': 'GP_player', 'GP_y': 'GP_team'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = merged_df.groupby('year').agg({\n",
    "    'o_pts': 'sum',\n",
    "    'o_fga': 'sum',\n",
    "    'o_oreb': 'sum',\n",
    "    'o_to': 'sum',\n",
    "    'o_fta': 'sum',\n",
    "    'o_asts': 'sum',\n",
    "    'o_fgm' : 'sum',\n",
    "    'o_ftm': 'sum',\n",
    "    'o_dreb':'sum',\n",
    "}).reset_index()\n",
    "\n",
    "grouped['VOP'] = grouped['o_pts'] / (grouped['o_fga'] - grouped['o_oreb'] + grouped['o_to'] + 0.44 * grouped['o_fta'])\n",
    "grouped['factor'] = (2 / 3) - (0.5 * (grouped['o_asts'] / grouped['o_fgm'])) / (2 * (grouped['o_fgm'] / grouped['o_ftm']))\n",
    "grouped['DRB%'] = (grouped['o_dreb'] - grouped['o_oreb']) / grouped['o_dreb']\n",
    "\n",
    "uPER_df = merged_df.groupby(['playerID', 'year']).agg({\n",
    "    'minutes': 'sum',     \n",
    "    'threeMade': 'sum',   \n",
    "    'assists': 'sum',     \n",
    "    'fgMade': 'sum',      \n",
    "    'ftMade': 'sum',      \n",
    "    'turnovers': 'sum',   \n",
    "    'fgAttempted': 'sum', \n",
    "    'ftAttempted': 'sum', \n",
    "    'dRebounds': 'sum',   \n",
    "    'oRebounds': 'sum',   \n",
    "    'steals': 'sum',      \n",
    "    'blocks': 'sum',      \n",
    "    'PF': 'sum'           \n",
    "}).reset_index()\n",
    "\n",
    "uPER_df = uPER_df.merge(grouped[['year', 'VOP', 'factor', 'DRB%']], on='year')\n",
    "\n",
    "uPER_df['TRB'] = uPER_df['dRebounds'] + uPER_df['oRebounds']\n",
    "\n",
    "uPER_df['uPER'] = (1 / uPER_df['minutes']) * (\n",
    "    uPER_df['threeMade'] +\n",
    "    (2/3) * uPER_df['assists'] +\n",
    "    (2 - uPER_df['factor'] * (uPER_df['assists'] / uPER_df['fgMade'])) * uPER_df['fgMade'] +\n",
    "    (uPER_df['ftMade'] * 0.5 * (1 + (1 - (uPER_df['assists'] / uPER_df['fgMade'])) + (2/3) * (uPER_df['assists'] / uPER_df['fgMade']))) -\n",
    "    uPER_df['VOP'] * uPER_df['turnovers'] -\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * (uPER_df['fgAttempted'] - uPER_df['fgMade']) -\n",
    "    uPER_df['VOP'] * 0.44 * (0.44 + (0.56 * uPER_df['DRB%'])) * (uPER_df['ftAttempted'] - uPER_df['ftMade']) +\n",
    "    uPER_df['VOP'] * (1 - uPER_df['DRB%']) * uPER_df['TRB'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['oRebounds'] +\n",
    "    uPER_df['VOP'] * uPER_df['steals'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['blocks'] -\n",
    "    uPER_df['PF'] * ((grouped['o_ftm'].mean() / grouped['o_pts'].mean()) - 0.44 * (grouped['o_fta'].mean() / grouped['o_pts'].mean()) * uPER_df['VOP'])\n",
    ")\n",
    "\n",
    "lg_uPER = uPER_df.groupby('year')['uPER'].mean().reset_index()\n",
    "lg_uPER.rename(columns={'uPER': 'lg_uPER'}, inplace=True)\n",
    "\n",
    "uPER_df = uPER_df.merge(lg_uPER, on='year')\n",
    "\n",
    "uPER_df['PER'] = uPER_df['uPER'] * (15 / uPER_df['lg_uPER'])\n",
    "\n",
    "print(uPER_df[['playerID', 'year', 'uPER', 'PER']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_to_merge = uPER_df[['playerID', 'year', 'PER']]\n",
    "merged_df = merged_df.merge(per_to_merge, on=['playerID', 'year'], how='left')\n",
    "\n",
    "merged_df['TS%'] = (merged_df['points'] / (2 * (merged_df['fgAttempted'] + 0.44 * merged_df['ftAttempted'])))*100\n",
    "merged_df['eFG%'] = ((merged_df['fgMade'] + 0.5 * merged_df['threeMade']) / merged_df['fgAttempted'])*100\n",
    "merged_df['stocks'] = (merged_df['steals'] + merged_df['blocks'])\n",
    "\n",
    "merged_df['PER'] = merged_df['PER'].fillna(0)\n",
    "merged_df['TS%'] = merged_df['TS%'].fillna(0)\n",
    "merged_df['eFG%'] = merged_df['eFG%'].fillna(0)\n",
    "merged_df['stocks'] = merged_df['stocks'].fillna(0)\n",
    "\n",
    "#Equipas que não foram aos playoffs\n",
    "merged_df['W'] = merged_df['W'].fillna(0)\n",
    "merged_df['L'] = merged_df['L'].fillna(0)\n",
    "\n",
    "\n",
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_for_each_column(dataset):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    if numeric_columns.empty:\n",
    "        print(\"No numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        numeric_columns.boxplot(figsize=(10, 6))\n",
    "        plt.title(\"Boxplot for all numeric columns\")\n",
    "        plt.xticks(rotation=45)  # Rotation in x, if necessary\n",
    "        plt.show()\n",
    "\n",
    "def pearson_correlation(dataset, size_x, size_y):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    \n",
    "    if numeric_columns.empty:\n",
    "        print(\"Nenhuma coluna numérica encontrada no dataset.\")\n",
    "    else:\n",
    "        # Correlation matrix\n",
    "        correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "        # View\n",
    "        plt.figure(figsize=(size_x, size_y))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Pearson-correlation')\n",
    "        plt.show()\n",
    "\n",
    "def bar_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            value_counts = non_numeric_columns[column].value_counts()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f\"Bar chart for '{column}'\")\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "            plt.show()\n",
    "\n",
    "# Pie-chart for each column\n",
    "def pie_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    \n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            # Count elements from different categories\n",
    "            category_counts = dataset[column].value_counts()\n",
    "            \n",
    "            # Pie-chart\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            category_counts.plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "            plt.title(f'Distribution of {column}')\n",
    "            plt.ylabel('')  # Remove o rótulo do eixo Y\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#box_plot_for_each_column(merged_df)\n",
    "#box_plot_for_each_column(coaches_file)\n",
    "#box_plot_for_each_column(series_post_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearson_correlation(merged_df, 100, 80)\n",
    "#pearson_correlation(coaches_file, 8, 6)\n",
    "#pearson_correlation(series_post_file, 8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar_chart_for_each_column(merged_df)\n",
    "#bar_chart_for_each_column(coaches_file)\n",
    "#bar_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie_chart_for_each_column(merged_df)\n",
    "#pie_chart_for_each_column(coaches_file)\n",
    "#pie_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceGameResults(column):\n",
    "    return column.apply(lambda value: '100' if value == 'W' else '010' if value == 'L' else '001')\n",
    "\n",
    "# Aplicar a função para cada coluna específica\n",
    "merged_df['firstRound'] = replaceGameResults(merged_df['firstRound'])\n",
    "merged_df['semis'] = replaceGameResults(merged_df['semis'])\n",
    "merged_df['finals'] = replaceGameResults(merged_df['finals'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df.to_csv('cleanDatasets/players_and_teams.csv', index=False)\n",
    "coaches_file.to_csv('cleanDatasets/coaches_and_awards.csv', index=False)\n",
    "series_post_file.to_csv('cleanDatasets/series_post.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = merged_df.drop(columns=['minutes','points','threeMade','assists','fgMade','turnovers','fgAttempted','ftAttempted','oRebounds','steals','blocks','PF','o_ftm','o_pts','o_fta','o_pts','o_fga','o_oreb','o_to','o_asts','o_fgm','o_dreb'])\n",
    "merged_df2 = merged_df2.drop(columns=['GP_player','GS','ftMade','threeAttempted','GP_team'])\n",
    "merged_df2 = merged_df2.drop(columns=['o_3pm','o_3pa','o_reb','o_pf','o_stl','o_blk','d_fgm','d_fga','d_ftm','d_fta','d_3pm','d_3pa','d_oreb','d_dreb','d_reb','d_asts','d_pf','d_stl','d_to','d_blk','d_pts'])\n",
    "merged_df2 = merged_df2.drop(columns=['PostGP','PostGS','PostMinutes','PostPoints','PostoRebounds','PostdRebounds','PostAssists','PostSteals','PostBlocks','PostTurnovers','PostPF','PostfgAttempted','PostfgMade','PostftAttempted','PostftMade','PostthreeAttempted','PostthreeMade','PostDQ'])\n",
    "merged_df2 = merged_df2.drop(columns=['arena'])\n",
    "\n",
    "merged_df2['birthDate'] = pd.to_datetime(merged_df['birthDate'], errors='coerce').dt.year\n",
    "merged_df2 = merged_df2.rename(columns={'birthDate': 'birthYear'})\n",
    "\n",
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df2.to_csv('cleanDatasets/advancedstatistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_stats_prevYear = merged_df2[['playerID','year','playoff','PER', 'eFG%', 'TS%','stocks','dRebounds']].drop_duplicates().copy()\n",
    "players_stats_prevYear['year'] = players_stats_prevYear['year'] + 1\n",
    "players_stats_prevYear = players_stats_prevYear.merge(\n",
    "    merged_df2[['playerID', 'year', 'tmID']], \n",
    "    on=['playerID', 'year'], \n",
    "    how='left')\n",
    "\n",
    "players_stats_prevYear.to_csv('cleanDatasets/players_stats_prevYear.csv', index=False)\n",
    "\n",
    "players_stats_prevYear.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula a média de PER, TS%, e eFG% por equipe e ano\n",
    "team_year_stats = players_stats_prevYear.groupby(['tmID', 'year'])[['PER', 'TS%', 'eFG%', 'stocks', 'dRebounds', 'playoff']].mean().reset_index()\n",
    "\n",
    "# Salva o novo dataset em um arquivo CSV\n",
    "team_year_stats.to_csv('cleanDatasets/team_year_stats.csv', index=False)\n",
    "\n",
    "# Visualizar os primeiros dados para verificar o resultado\n",
    "team_year_stats.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir o dataset para treino, validacao e teste \n",
    "\n",
    "dataset_treino = team_year_stats[(team_year_stats['year'] >= 1) & (team_year_stats['year'] <= 7)]\n",
    "dataset_validacao = team_year_stats[team_year_stats['year'].isin([8, 9])]\n",
    "dataset_teste = team_year_stats[team_year_stats['year'] == 10]\n",
    "\n",
    "dataset_teste.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Features (X) e alvo (y)\n",
    "X_treino = dataset_treino[['PER', 'TS%', 'eFG%']]\n",
    "y_treino = dataset_treino['playoff']\n",
    "\n",
    "X_validacao = dataset_validacao[['PER', 'TS%', 'eFG%']]\n",
    "y_validacao = dataset_validacao['playoff']\n",
    "\n",
    "X_teste = dataset_teste[['PER', 'TS%', 'eFG%']]\n",
    "\n",
    "# Inicializar o modelo de regressão\n",
    "modelo = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Treinar com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Validar o modelo\n",
    "y_pred_validacao = modelo.predict(X_validacao)\n",
    "\n",
    "# Avaliar o modelo com métricas de regressão\n",
    "mse = mean_squared_error(y_validacao, y_pred_validacao)\n",
    "r2 = r2_score(y_validacao, y_pred_validacao)\n",
    "\n",
    "print(\"Mean Squared Error (MSE) na validação:\", mse)\n",
    "print(\"R² score na validação:\", r2)\n",
    "\n",
    "# Adicionar as predições ao DataFrame de validação\n",
    "dataset_validacao['pred_playoff'] = y_pred_validacao\n",
    "\n",
    "# Exibir os primeiros registros para conferência\n",
    "print(dataset_validacao)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
