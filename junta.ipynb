{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df1 = pd.read_csv('datasets/players.csv')\n",
    "df2 = pd.read_csv('datasets/players_teams.csv')\n",
    "df3 = pd.read_csv('datasets/awards_players.csv')\n",
    "df4 = pd.read_csv('datasets/teams.csv')\n",
    "df5 = pd.read_csv('datasets/teams_post.csv')\n",
    "df6 = pd.read_csv('datasets/coaches.csv')\n",
    "df7 = pd.read_csv('datasets/series_post.csv')\n",
    "\n",
    "def corrige_vencedor(teams, series_post):\n",
    "    # Itera sobre cada rodada ('F', 'CF', 'FR') para ajustar cada fase dos playoffs\n",
    "    for round_type in ['FR', 'CF', 'F']:\n",
    "        # Filtra a série específica da rodada\n",
    "        series_round = series_post[series_post['round'] == round_type]\n",
    "        \n",
    "        # Atualiza cada série individualmente\n",
    "        for _, row in series_round.iterrows():\n",
    "            year = row['year']\n",
    "            winner_id = row['tmIDWinner']\n",
    "            loser_id = row['tmIDLoser']\n",
    "            \n",
    "            # Define as colunas que correspondem às rodadas\n",
    "            if round_type == 'FR':\n",
    "                round_column = 'firstRound'\n",
    "            elif round_type == 'CF':\n",
    "                round_column = 'semis'\n",
    "            elif round_type == 'F':\n",
    "                round_column = 'finals'\n",
    "            \n",
    "            # Marca o time vencedor como \"W\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == winner_id), round_column] = 'W'\n",
    "            \n",
    "            # Marca o time perdedor como \"L\" na rodada correspondente\n",
    "            teams.loc[(teams['year'] == year) & (teams['tmID'] == loser_id), round_column] = 'L'\n",
    "    \n",
    "    return teams\n",
    "\n",
    "teams_file = corrige_vencedor(df4, df7)\n",
    "\n",
    "players_teams_file = df2.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "\n",
    "players_file = df1[df1['pos'].notna() & (df1['pos'] != '')] # tirar jogadoras com linhas vazias\n",
    "\n",
    "players_file = players_file.drop(columns=['firstseason', 'lastseason', 'deathDate', 'collegeOther']) # dropar firstseason e lastseason porque têm os valores todos iguais, dropar deathDate porque quase ninguém morreu \n",
    "players_file['college'] = players_file['college'].apply(lambda x: 1 if pd.notnull(x) else 0) # substituir college por escolaridade\n",
    "\n",
    "merged_df = pd.merge(players_teams_file, players_file, left_on='playerID', right_on='bioID', how='left') # merge players_teams e players\n",
    "\n",
    "merged_df = merged_df.drop(columns=['bioID']) # tirar bioID porque já temos playerID\n",
    "awards_players_file = df3.drop(columns=['lgID']) # tirar porque é tudo igual\n",
    "teams_file = df4.drop(columns=['lgID', 'divID', 'tmORB','tmDRB','tmTRB','opptmORB','opptmDRB','opptmTRB','seeded']) # dropar lgID e divID porque é igual em tudo e o resto estava tudo a zero\n",
    "teams_file['playoff'] = teams_file['playoff'].apply(lambda x: 1 if x=='Y' else 0) # substituir playoff por valores numericos\n",
    "\n",
    "team_post_file = df5.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "series_post_file = df7.drop(columns=['lgIDWinner', 'lgIDLoser']) # tirar tudo porque é tudo igual\n",
    "coaches_file = df6.drop(columns=['lgID']) # tirar lgID de tudo porque é tudo igual\n",
    "\n",
    "\n",
    "awards_grouped = awards_players_file.groupby(['playerID', 'year'])['award'].apply(list).reset_index() # agrupamos numa lista todos os awards de cada jogadora em cada ano\n",
    "awards_grouped['award'] = awards_grouped['award'].apply(lambda x: x if isinstance(x, list) else []) # se uma jogadora não tem awards -> lista vazia\n",
    "\n",
    "merged_df = pd.merge(merged_df, awards_grouped, on=['playerID', 'year'], how='left') # merge do dataset que foi merged em cima com o dos awards\n",
    "merged_df['award'] = merged_df['award'].apply(lambda x: x if isinstance(x, list) else []) # se uma jogadora não tem awards -> lista vazia\n",
    "merged_df = pd.merge(merged_df, teams_file, on=['tmID','year'], how = 'left') # merge com o teams\n",
    "\n",
    "merged_df = merged_df.drop(columns=['franchID', 'name']) # dropar franchID porque é igual à sigla da equipa e name porque já temos a sigla\n",
    "\n",
    "merged_df = pd.merge(merged_df, team_post_file, on=['tmID','year'], how = 'left') # merge com o teams post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = pd.read_csv('datasets/teams.csv')\n",
    "\n",
    "# Transformar os dados de rodadas em formato longo\n",
    "winner_counts = teams.melt(\n",
    "    id_vars=['year', 'tmID'],  # Colunas fixas\n",
    "    value_vars=['firstRound', 'semis', 'finals'],  # Colunas que serão transformadas\n",
    "    var_name='round',  # Nome para a coluna das rodadas\n",
    "    value_name='result'  # Nome para a coluna dos resultados\n",
    ")\n",
    "\n",
    "# Filtrar apenas as equipes vencedoras\n",
    "winner_counts = winner_counts[winner_counts['result'] == 'W']\n",
    "\n",
    "# Garantir a ordem correta das rodadas\n",
    "round_order = ['firstRound', 'semis', 'finals']\n",
    "winner_counts['round'] = pd.Categorical(winner_counts['round'], categories=round_order, ordered=True)\n",
    "\n",
    "# Contar o número de vencedores por rodada e ano\n",
    "winner_summary = winner_counts.groupby(['year', 'round']).size().reset_index(name='winner_count')\n",
    "\n",
    "# Criar o gráfico\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Cores para cada rodada\n",
    "round_colors = {\n",
    "    'firstRound': sns.color_palette('Set3')[0],\n",
    "    'semis': sns.color_palette('Set3')[1],\n",
    "    'finals': sns.color_palette('Set3')[2]\n",
    "}\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "sns.barplot(data=winner_summary, x='year', y='winner_count', hue='round', hue_order=round_order, palette=round_colors)\n",
    "\n",
    "# Adicionar linhas de valor esperado da mesma cor das barras\n",
    "plt.axhline(4, color=round_colors['firstRound'], linestyle='--', label='Valor esperado (4 vencedores - First Round)')\n",
    "plt.axhline(2, color=round_colors['semis'], linestyle='--', label='Valor esperado (2 vencedores - Semis)')\n",
    "plt.axhline(1, color=round_colors['finals'], linestyle='--', label='Valor esperado (1 vencedor - Finals)')\n",
    "\n",
    "# Personalizar o gráfico\n",
    "plt.title('Número de vencedores por ronda e ano')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Número de vencedores')\n",
    "plt.legend(title='Expectativa por ronda')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Exibir o gráfico\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate min, max, and average player count for each year\n",
    "\n",
    "player_count_per_team_year = merged_df.groupby(['year', 'tmID'])['playerID'].nunique().reset_index()\n",
    "player_count_per_team_year.columns = ['Year', 'Team', 'PlayerCount']\n",
    "\n",
    "# print(player_count_per_team_year)\n",
    "\n",
    "summary_stats = player_count_per_team_year.groupby('Year')['PlayerCount'].agg(['min', 'max', 'mean']).reset_index()\n",
    "summary_stats.columns = ['Year', 'MinPlayerCount', 'MaxPlayerCount', 'AvgPlayerCount']\n",
    "\n",
    "summary_stats.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge do awards com os coaches\n",
    "\n",
    "awards_coaches_file = df3.rename(columns={'playerID': 'coachID'})\n",
    "coach_awards = awards_coaches_file[awards_coaches_file['award'] == 'Coach of the Year']\n",
    "coach_awards_grouped = coach_awards.groupby(['coachID', 'year'])['award'].apply(list).reset_index()\n",
    "coaches_file = pd.merge(coaches_file, coach_awards_grouped, on=['coachID', 'year'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average rebounds by position, excluding secondary positions\n",
    "avg_rebounds_by_pos_filtered = (\n",
    "    merged_df.groupby('pos')\n",
    "    .agg(oRebounds=('oRebounds', 'mean'), dRebounds=('dRebounds', 'mean'))\n",
    "    .reset_index()\n",
    "    .query(\"pos in ['G', 'F', 'C']\")\n",
    ")\n",
    "\n",
    "avg_rebounds_by_pos_filtered['d_over_o_reb'] = avg_rebounds_by_pos_filtered['dRebounds'] / avg_rebounds_by_pos_filtered['oRebounds']\n",
    "avg_rebounds_by_pos_filtered.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.drop(columns=['rebounds', 'PostRebounds']) # resultado da célula anterior, os rebounds ofensivos/defensivos mudam de acordo com a posição da jogadora, logo decidimos tirar o total de rebounds\n",
    "merged_df = merged_df.rename(columns={'GP_x': 'GP_player', 'GP_y': 'GP_team'}) # haviam duas colunas com nomes iguais, uma para as jogadoras e outra para as equipas, tinham ficado uma com x e a outra com y então demos rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar Novas estatisticas -> PER (Player Effiency Rating)\n",
    "\n",
    "grouped = merged_df.groupby('year').agg({\n",
    "    'o_pts': 'sum',\n",
    "    'o_fga': 'sum',\n",
    "    'o_oreb': 'sum',\n",
    "    'o_to': 'sum',\n",
    "    'o_fta': 'sum',\n",
    "    'o_asts': 'sum',\n",
    "    'o_fgm' : 'sum',\n",
    "    'o_ftm': 'sum',\n",
    "    'o_dreb':'sum',\n",
    "}).reset_index()\n",
    "\n",
    "grouped['VOP'] = grouped['o_pts'] / (grouped['o_fga'] - grouped['o_oreb'] + grouped['o_to'] + 0.44 * grouped['o_fta'])\n",
    "grouped['factor'] = (2 / 3) - (0.5 * (grouped['o_asts'] / grouped['o_fgm'])) / (2 * (grouped['o_fgm'] / grouped['o_ftm']))\n",
    "grouped['DRB%'] = (grouped['o_dreb'] - grouped['o_oreb']) / grouped['o_dreb']\n",
    "\n",
    "uPER_df = merged_df.groupby(['playerID', 'year']).agg({\n",
    "    'minutes': 'sum',     \n",
    "    'threeMade': 'sum',   \n",
    "    'assists': 'sum',     \n",
    "    'fgMade': 'sum',      \n",
    "    'ftMade': 'sum',      \n",
    "    'turnovers': 'sum',   \n",
    "    'fgAttempted': 'sum', \n",
    "    'ftAttempted': 'sum', \n",
    "    'dRebounds': 'sum',   \n",
    "    'oRebounds': 'sum',   \n",
    "    'steals': 'sum',      \n",
    "    'blocks': 'sum',      \n",
    "    'PF': 'sum'           \n",
    "}).reset_index()\n",
    "\n",
    "uPER_df = uPER_df.merge(grouped[['year', 'VOP', 'factor', 'DRB%']], on='year')\n",
    "\n",
    "uPER_df['TRB'] = uPER_df['dRebounds'] + uPER_df['oRebounds']\n",
    "\n",
    "uPER_df['uPER'] = (1 / uPER_df['minutes']) * (\n",
    "    uPER_df['threeMade'] +\n",
    "    (2/3) * uPER_df['assists'] +\n",
    "    (2 - uPER_df['factor'] * (uPER_df['assists'] / uPER_df['fgMade'])) * uPER_df['fgMade'] +\n",
    "    (uPER_df['ftMade'] * 0.5 * (1 + (1 - (uPER_df['assists'] / uPER_df['fgMade'])) + (2/3) * (uPER_df['assists'] / uPER_df['fgMade']))) -\n",
    "    uPER_df['VOP'] * uPER_df['turnovers'] -\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * (uPER_df['fgAttempted'] - uPER_df['fgMade']) -\n",
    "    uPER_df['VOP'] * 0.44 * (0.44 + (0.56 * uPER_df['DRB%'])) * (uPER_df['ftAttempted'] - uPER_df['ftMade']) +\n",
    "    uPER_df['VOP'] * (1 - uPER_df['DRB%']) * uPER_df['TRB'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['oRebounds'] +\n",
    "    uPER_df['VOP'] * uPER_df['steals'] +\n",
    "    uPER_df['VOP'] * uPER_df['DRB%'] * uPER_df['blocks'] -\n",
    "    uPER_df['PF'] * ((grouped['o_ftm'].mean() / grouped['o_pts'].mean()) - 0.44 * (grouped['o_fta'].mean() / grouped['o_pts'].mean()) * uPER_df['VOP'])\n",
    ")\n",
    "\n",
    "lg_uPER = uPER_df.groupby('year')['uPER'].mean().reset_index()\n",
    "lg_uPER.rename(columns={'uPER': 'lg_uPER'}, inplace=True)\n",
    "\n",
    "uPER_df = uPER_df.merge(lg_uPER, on='year')\n",
    "\n",
    "uPER_df['PER'] = uPER_df['uPER'] * (15 / uPER_df['lg_uPER'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_to_merge = uPER_df[['playerID', 'year', 'PER']]\n",
    "merged_df = merged_df.merge(per_to_merge, on=['playerID', 'year'], how='left') # adicionar o PER ao dataset\n",
    "\n",
    "# novas estatisticas\n",
    "merged_df['TS%'] = (merged_df['points'] / (2 * (merged_df['fgAttempted'] + 0.44 * merged_df['ftAttempted'])))*100\n",
    "merged_df['eFG%'] = ((merged_df['fgMade'] + 0.5 * merged_df['threeMade']) / merged_df['fgAttempted'])*100\n",
    "merged_df['stocks'] = (merged_df['steals'] + merged_df['blocks'])\n",
    "merged_df['dar'] = ((merged_df['steals'] + merged_df['blocks'] + merged_df['oRebounds'] + merged_df['dRebounds'])/merged_df['minutes'])\n",
    "\n",
    "# substituir por 0 colunas vazias\n",
    "merged_df['PER'] = merged_df['PER'].fillna(0)\n",
    "merged_df['TS%'] = merged_df['TS%'].fillna(0)\n",
    "merged_df['eFG%'] = merged_df['eFG%'].fillna(0)\n",
    "merged_df['stocks'] = merged_df['stocks'].fillna(0)\n",
    "merged_df['dar'] = merged_df['dar'].fillna(0)\n",
    "\n",
    "#Equipas que não foram aos playoffs\n",
    "merged_df['W'] = merged_df['W'].fillna(0)\n",
    "merged_df['L'] = merged_df['L'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_plot_for_each_column(dataset):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    if numeric_columns.empty:\n",
    "        print(\"No numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        numeric_columns.boxplot(figsize=(10, 6))\n",
    "        plt.title(\"Boxplot for all numeric columns\")\n",
    "        plt.xticks(rotation=45)  # Rotation in x, if necessary\n",
    "        plt.show()\n",
    "\n",
    "def pearson_correlation(dataset, size_x, size_y):\n",
    "    numeric_columns = dataset.select_dtypes(include='number')\n",
    "    \n",
    "    if numeric_columns.empty:\n",
    "        print(\"Nenhuma coluna numérica encontrada no dataset.\")\n",
    "    else:\n",
    "        # Correlation matrix\n",
    "        correlation_matrix = numeric_columns.corr()\n",
    "\n",
    "        # View\n",
    "        plt.figure(figsize=(size_x, size_y))\n",
    "        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "        plt.title('Pearson-correlation')\n",
    "        plt.show()\n",
    "\n",
    "def bar_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            value_counts = non_numeric_columns[column].value_counts()\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            value_counts.plot(kind='bar')\n",
    "            plt.title(f\"Bar chart for '{column}'\")\n",
    "            plt.xlabel(column)\n",
    "            plt.ylabel(\"Count\")\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "            plt.show()\n",
    "\n",
    "# Pie-chart for each column\n",
    "def pie_chart_for_each_column(dataset):\n",
    "    non_numeric_columns = dataset.select_dtypes(exclude='number')\n",
    "    \n",
    "    if non_numeric_columns.empty:\n",
    "        print(\"Any non-numeric columns found in the dataset.\")\n",
    "    else:\n",
    "        for column in non_numeric_columns.columns:\n",
    "            # Count elements from different categories\n",
    "            category_counts = dataset[column].value_counts()\n",
    "            \n",
    "            # Pie-chart\n",
    "            plt.figure(figsize=(6, 6))\n",
    "            category_counts.plot.pie(autopct='%1.1f%%', startangle=140)\n",
    "            plt.title(f'Distribution of {column}')\n",
    "            plt.ylabel('')  # Remove o rótulo do eixo Y\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "#box_plot_for_each_column(merged_df)\n",
    "#box_plot_for_each_column(coaches_file)\n",
    "#box_plot_for_each_column(series_post_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pearson_correlation(merged_df, 100, 80)\n",
    "#pearson_correlation(coaches_file, 8, 6)\n",
    "#pearson_correlation(series_post_file, 8, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bar_chart_for_each_column(merged_df)\n",
    "#bar_chart_for_each_column(coaches_file)\n",
    "#bar_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pie_chart_for_each_column(merged_df)\n",
    "#pie_chart_for_each_column(coaches_file)\n",
    "#pie_chart_for_each_column(series_post_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Substituir valores nominais para valores relativos (one-attribute-per-value conversion)\n",
    "def replaceGameResults(column):\n",
    "    return column.apply(lambda value: '100' if value == 'W' else '010' if value == 'L' else '001')\n",
    "\n",
    "# Aplicar a função para cada coluna específica\n",
    "merged_df['firstRound'] = replaceGameResults(merged_df['firstRound'])\n",
    "merged_df['semis'] = replaceGameResults(merged_df['semis'])\n",
    "merged_df['finals'] = replaceGameResults(merged_df['finals'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar os datasets numa pasta\n",
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df.to_csv('cleanDatasets/players_and_teams.csv', index=False)\n",
    "coaches_file.to_csv('cleanDatasets/coaches_and_awards.csv', index=False)\n",
    "series_post_file.to_csv('cleanDatasets/series_post.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queriamos construir o dataset em relação às jogadoras então dropamos imensas colunas relativas à equipa\n",
    "\n",
    "merged_df2 = merged_df.drop(columns=['minutes','threeMade','assists','fgMade','turnovers','fgAttempted','ftAttempted','oRebounds','steals','blocks','PF','o_ftm','o_pts','o_fta','o_pts','o_fga','o_oreb','o_to','o_asts','o_fgm','o_dreb']) # estes atributos já se encontram nas novas colunas criadas\n",
    "merged_df2 = merged_df2.drop(columns=['GP_player','GS','ftMade','threeAttempted','GP_team'])\n",
    "merged_df2 = merged_df2.drop(columns=['o_3pm','o_3pa','o_reb','o_pf','o_stl','o_blk','d_fgm','d_fga','d_ftm','d_fta','d_3pm','d_3pa','d_oreb','d_dreb','d_reb','d_asts','d_pf','d_stl','d_to','d_blk','d_pts'])\n",
    "merged_df2 = merged_df2.drop(columns=['PostGP','PostGS','PostMinutes','PostPoints','PostoRebounds','PostdRebounds','PostAssists','PostSteals','PostBlocks','PostTurnovers','PostPF','PostfgAttempted','PostfgMade','PostftAttempted','PostftMade','PostthreeAttempted','PostthreeMade','PostDQ'])\n",
    "merged_df2 = merged_df2.drop(columns=['arena'])\n",
    "\n",
    "# substituir BirthDate por ano em que nasceram\n",
    "merged_df2['birthDate'] = pd.to_datetime(merged_df['birthDate'], errors='coerce').dt.year\n",
    "merged_df2 = merged_df2.rename(columns={'birthDate': 'birthYear'})\n",
    "\n",
    "if not os.path.exists('cleanDatasets'):\n",
    "    os.makedirs('cleanDatasets')\n",
    "\n",
    "merged_df2.to_csv('cleanDatasets/advancedstatistics.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Juntar os treinadores\n",
    "\n",
    "\n",
    "# Agrupar coaches por year e team, transformando em listas\n",
    "coaches_grouped = df6.groupby(['year', 'tmID'])['coachID'].apply(list).reset_index()\n",
    "\n",
    "# Fazer o merge com o ficheiro\n",
    "merged_df2 = merged_df2.merge(coaches_grouped, on=['year', 'tmID'], how='left')\n",
    "\n",
    "merged_df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_stats_prevYear = merged_df2[['playerID','year','PER', 'eFG%', 'TS%','stocks','dRebounds','dar']].drop_duplicates().copy() #TODO Acrescentar aqui mais variaveis\n",
    "\n",
    "#players_stats_prevYear['legacy_points'] = merged_df2['points']\n",
    "players_stats_prevYear['legacy_points'] = merged_df2['won']\n",
    "players_stats_prevYear['year'] = players_stats_prevYear['year'] + 1\n",
    "\n",
    "\n",
    "players_stats_prevYear = players_stats_prevYear.merge(\n",
    "    merged_df2[['playerID', 'year', 'tmID', 'playoff','coachID']], \n",
    "    on=['playerID', 'year'], \n",
    "    how='left')\n",
    "\n",
    "players_stats_prevYear.to_csv('cleanDatasets/players_stats_prevYear.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................Fazer a media por equipa dos valores mas pegando apenas nos 7 melhores jogadores...............\n",
    "\n",
    "# Ordenar os jogadores dentro de cada equipe e ano com base no PER (ou outra métrica)\n",
    "players_stats_prevYear_sorted = players_stats_prevYear.sort_values(by=['tmID', 'year', 'PER'], ascending=[True, True, False])\n",
    "\n",
    "# Selecionar os 5 melhores jogadores de cada equipe e ano\n",
    "top_7_players = players_stats_prevYear_sorted.groupby(['tmID', 'year']).head(5)\n",
    "\n",
    "# Agora, calcular a média das métricas apenas para os melhores jogadores\n",
    "team_year_stats = top_7_players.groupby(['tmID', 'year', 'playoff']).agg({\n",
    "    'PER': 'mean',\n",
    "    'TS%': 'mean',\n",
    "    'eFG%': 'mean',\n",
    "    'stocks': 'mean',\n",
    "    'dRebounds': 'mean',\n",
    "    'dar': 'mean',\n",
    "    'coachID': 'first',  # Seleciona o primeiro valor da coluna (presumindo que todos sejam iguais para o grupo)\n",
    "    'legacy_points' : 'mean' #Same here\n",
    "}).reset_index()\n",
    "\n",
    "# Salva o novo dataset em um arquivo CSV\n",
    "team_year_stats.to_csv('cleanDatasets/team_year_stats.csv', index=False)\n",
    "\n",
    "team_year_stats.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Para depois poder ir buscar as variavies categoricas\n",
    "team_year_stats_copy = team_year_stats.copy()\n",
    "\n",
    "\n",
    "# Selecionar as colunas de interesse\n",
    "features = team_year_stats[['PER', 'TS%', 'eFG%','stocks','dRebounds']] #TODO Acrescentar aqui tambem\n",
    "\n",
    "# Normalizar os dados usando MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "features_scaled = scaler.fit_transform(features)\n",
    "\n",
    "# Aplicar o PCA\n",
    "pca = PCA(n_components=3) #TODO mudar aqui o numero de colunas a selecionar\n",
    "pca.fit(features_scaled)\n",
    "\n",
    "# Verificar as cargas (coeficientes) dos componentes principais\n",
    "components = pca.components_\n",
    "\n",
    "# Baseado nas cargas, você pode decidir as variáveis mais importantes\n",
    "# Vamos mostrar a importância de cada variável nas componentes principais\n",
    "\n",
    "# Calcular a soma das cargas absolutas para cada variável\n",
    "importance = pd.DataFrame(abs(components), columns=['PER', 'TS%', 'eFG%','stocks','dRebounds'], index=['PC1', 'PC2','PC3']) #TODO mudar aqui as variaveis\n",
    "#TODO meter tantos PC quanto variaves a selecionar\n",
    "importance_sum = importance.sum(axis=0)\n",
    "\n",
    "# Selecionar as duas variáveis mais importantes\n",
    "most_important_features = importance_sum.sort_values(ascending=False).head(3) #TODO mudar aqui o numero de variaveis\n",
    "\n",
    "# Exibir apenas os nomes das variáveis mais importantes\n",
    "important_variable_names = most_important_features.index.tolist()\n",
    "\n",
    "# Inicializar a lista de componentes a serem removidos\n",
    "components_to_drop = ['PER', 'TS%', 'eFG%','stocks','dRebounds','dar'] #TODO mudar aqui as variavies (TODOS OS ATRIBUTOS AQUI)\n",
    "#TODO nao esquecer de aqui adicionar as nao numericas, como os coaches\n",
    "\n",
    "# Remover as variáveis mais importantes da lista de componentes a serem removidos\n",
    "components_to_drop = [col for col in components_to_drop if col not in important_variable_names]\n",
    "\n",
    "team_year_stats=team_year_stats.drop(columns=components_to_drop)\n",
    "\n",
    "#Adicionar as variaveis categoricas, como os coaches, que nao passaram no processo de PCA\n",
    "team_year_stats['coachID'] = team_year_stats_copy['coachID']\n",
    "#team_year_stats['legacy_points'] = team_year_stats_copy['legacy_points']\n",
    "#important_variable_names.append('legacy_points')\n",
    "\n",
    "# Exibir o resultado\n",
    "print(team_year_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizar os dados \n",
    "\n",
    "# Selecionar apenas colunas numéricas\n",
    "# Lista de colunas a normalizar\n",
    "columns_to_normalize = ['PER', 'eFG%', 'stocks','legacy_points'] #TODO baseado nas colunas selecionadas do dataset de cima\n",
    "\n",
    "# Aplicar a normalização apenas nas colunas selecionadas\n",
    "team_year_stats[columns_to_normalize] = team_year_stats[columns_to_normalize].apply(\n",
    "    lambda x: (x - x.min()) / (x.max() - x.min())\n",
    ")\n",
    "\n",
    "# Verificar o resultado\n",
    "print(team_year_stats.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #Passar as colunas que nao sao numericas para numericas\n",
    "# One-Hot Encoding e é útil para transformar colunas categóricas em representações numéricas binárias.\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "# Codificando a lista de coaches em colunas binárias\n",
    "coach_dummies = mlb.fit_transform(team_year_stats['coachID'])\n",
    "\n",
    "# Adicionando as novas colunas no dataframe original\n",
    "coach_columns = mlb.classes_\n",
    "team_year_stats = team_year_stats.join(pd.DataFrame(coach_dummies, columns=coach_columns))\n",
    "\n",
    "# Adicionando os nomes das novas colunas à lista de variáveis importantes\n",
    "important_variable_names.extend(coach_columns) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir o dataset para treino, validacao e teste \n",
    "\n",
    "dataset_treino = team_year_stats[(team_year_stats['year'] >= 7) & (team_year_stats['year'] <= 9)]\n",
    "dataset_teste = team_year_stats[team_year_stats['year'] == 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Features (X) e alvo (y)\n",
    "X_treino = dataset_treino[important_variable_names] #Antigamente 'PER', 'TS%', 'eFG%'\n",
    "y_treino = dataset_treino['playoff']\n",
    "\n",
    "X_teste = dataset_teste[important_variable_names]\n",
    "y_teste = dataset_teste['playoff']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criar ficheiros para guardar resultados e avaliar\n",
    "dataset_resultados = pd.DataFrame({})\n",
    "\n",
    "dataset_resultados[\"playoff\"] = dataset_teste[\"playoff\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo de regressão linear\n",
    "modelo = RandomForestRegressor()\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"RandomForestRegressor\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo de regressão linear\n",
    "modelo = LinearRegression()\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"LinearRegression\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#::::::::::::::::::::Este modelo é uma variante da regressão linear que usa regularização L2 para reduzir overfitting.::::::::::::::::\n",
    "\n",
    "# Inicializar o modelo de Ridge Regression\n",
    "modelo = Ridge(alpha=1.0)  # alpha controla o nível de regularização\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"Ridge\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar ao Ridge, mas utiliza regularização L1. Tende a eliminar variáveis menos importantes, útil para seleção de features.\n",
    "# Inicializar o modelo de Lasso Regression\n",
    "modelo = Lasso(alpha=0.1, random_state=42)  # alpha controla a força da regularização\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combina as regularizações L1 (Lasso) e L2 (Ridge).\n",
    "# Inicializar o modelo de Elastic Net\n",
    "modelo = ElasticNet(alpha=0.1, l1_ratio=0.5, random_state=42)\n",
    "# `alpha` controla a força total da regularização.\n",
    "# `l1_ratio` controla a proporção de regularização L1 (Lasso) em relação à L2 (Ridge).\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "# Exibir as primeiras linhas para verificar\n",
    "dataset_teste.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo de MLP\n",
    "modelo = MLPRegressor(hidden_layer_sizes=(100, 50),  # Camadas ocultas com 100 e 50 neurônios\n",
    "                      activation='relu',            # Função de ativação\n",
    "                      solver='adam',                # Otimizador\n",
    "                      max_iter=500,                 # Número máximo de iterações\n",
    "                      random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "#Adicioanar a coluna aos resultados\n",
    "dataset_resultados[\"MLPRegressor\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inicializar o modelo sequencial\n",
    "modelo = Sequential()\n",
    "\n",
    "# Adicionar camadas\n",
    "modelo.add(Dense(64, activation='relu', input_shape=(X_treino.shape[1],)))  # Primeira camada oculta\n",
    "modelo.add(Dense(32, activation='relu'))  # Segunda camada oculta\n",
    "modelo.add(Dense(1))  # Camada de saída (regressão)\n",
    "\n",
    "# Compilar o modelo\n",
    "modelo.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino, epochs=50, batch_size=32, verbose=1, validation_split=0.2)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste).flatten()  # Flatten para transformar em 1D\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"Sequential\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo de regressão com Extra Trees\n",
    "modelo = ExtraTreesRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"ExtraTreesRegressor\"] = y_pred_teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo ElasticNet\n",
    "modelo = ElasticNet(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"ElasticNet\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo XGBoost Regressor\n",
    "modelo = xgb.XGBRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"XGBRegressor\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo LightGBM Regressor\n",
    "modelo = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "dataset_resultados[\"LGBMRegressor\"] = y_pred_teste\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o modelo de regressão com Gradient Boosting\n",
    "modelo = GradientBoostingRegressor(random_state=42) #TODO porque foi o melhor modelo até agora\n",
    "\n",
    "# Treinar o modelo com os dados de treino\n",
    "modelo.fit(X_treino, y_treino)\n",
    "\n",
    "# Fazer previsões no conjunto de teste\n",
    "y_pred_teste = modelo.predict(X_teste)\n",
    "\n",
    "# Adicionar a coluna prevista ao dataset_teste\n",
    "dataset_teste['playoff_previsto'] = y_pred_teste\n",
    "\n",
    "# Critério de correção\n",
    "# 1. Quando 'playoff' é 1, 'playoff_previsto' deve ser > 0.5\n",
    "# 2. Quando 'playoff' é 0, 'playoff_previsto' deve ser <= 0.5\n",
    "\n",
    "# Criar uma coluna indicando se a previsão está correta\n",
    "dataset_teste['correto'] = ((dataset_teste['playoff'] == 1) & (dataset_teste['playoff_previsto'] > 0.5)) | \\\n",
    "                           ((dataset_teste['playoff'] == 0) & (dataset_teste['playoff_previsto'] <= 0.5))\n",
    "\n",
    "# Contar o número de previsões corretas\n",
    "corretos = dataset_teste['correto'].sum()\n",
    "\n",
    "# Número total de exemplos\n",
    "total = len(dataset_teste)\n",
    "\n",
    "# Exibir o resultado\n",
    "print(f\"Previsões corretas: {corretos} de {total} ({(corretos / total) * 100:.2f}%)\")\n",
    "\n",
    "dataset_resultados[\"GradientBoostingRegressor\"] = y_pred_teste\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados considerando que 0.5 para cima é 1 e 0.5 para baixo da 0\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame(dataset_resultados)\n",
    "\n",
    "# Inicializar lista para armazenar os resultados\n",
    "resultados = []\n",
    "\n",
    "# Loop sobre cada coluna do modelo (excluindo a coluna 'playoff')\n",
    "for modelo in df.columns[1:]:\n",
    "    # Cálculo de métricas de regressão\n",
    "    mse = mean_squared_error(df[\"playoff\"], df[modelo])\n",
    "    mae = mean_absolute_error(df[\"playoff\"], df[modelo])\n",
    "    r2 = r2_score(df[\"playoff\"], df[modelo])\n",
    "    \n",
    "    # Binarização dos valores (0.5 como limite)\n",
    "    y_true = (df[\"playoff\"] >= 0.5).astype(int)\n",
    "    y_pred = (df[modelo] >= 0.5).astype(int)\n",
    "    \n",
    "    # Cálculo de métricas de classificação\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Adicionar os resultados à lista\n",
    "    resultados.append({\n",
    "        \"Modelo\": modelo,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "resultados_cima_baixo = pd.DataFrame(resultados)\n",
    "\n",
    "# Exibir os resultados\n",
    "resultados_cima_baixo.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resultados considerando que ficam a 1 os 8 melhores\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, mean_squared_error, r2_score, mean_absolute_error\n",
    "import pandas as pd\n",
    "\n",
    "# Criando o DataFrame\n",
    "df = pd.DataFrame(dataset_resultados)\n",
    "\n",
    "# Inicializar lista para armazenar os resultados\n",
    "resultados = []\n",
    "\n",
    "# Número de melhores previsões para considerar como 1\n",
    "top_n = 8\n",
    "\n",
    "# Loop sobre cada coluna do modelo (excluindo a coluna 'playoff')\n",
    "for modelo in df.columns[1:]:\n",
    "    # Cálculo de métricas de regressão\n",
    "    mse = mean_squared_error(df[\"playoff\"], df[modelo])\n",
    "    mae = mean_absolute_error(df[\"playoff\"], df[modelo])\n",
    "    r2 = r2_score(df[\"playoff\"], df[modelo])\n",
    "    \n",
    "    # Binarização dos valores (top 8 melhores como 1)\n",
    "    y_true = (df[\"playoff\"] >= 0.5).astype(int)\n",
    "    y_pred = pd.Series(0, index=df.index)  # Inicializa com 0 para todos\n",
    "    top_indices = df[modelo].nlargest(top_n).index  # Obtém índices dos top 8 valores\n",
    "    y_pred.loc[top_indices] = 1  # Define 1 para os top 8 valores\n",
    "    \n",
    "    # Cálculo de métricas de classificação\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Adicionar os resultados à lista\n",
    "    resultados.append({\n",
    "        \"Modelo\": modelo,\n",
    "        \"MSE\": mse,\n",
    "        \"MAE\": mae,\n",
    "        \"R²\": r2,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"Accuracy\": accuracy\n",
    "    })\n",
    "\n",
    "# Converter os resultados em um DataFrame\n",
    "resultados_8_melhores = pd.DataFrame(resultados)\n",
    "\n",
    "# Exibir os resultados\n",
    "resultados_8_melhores.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
